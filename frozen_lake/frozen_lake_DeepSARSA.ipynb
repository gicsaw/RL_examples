{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "lr=0.001\n",
    "lr2=1.0\n",
    "gamma=0.99\n",
    "epsilon0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4\n",
      "WARNING:tensorflow:From /home/shade/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "inputdim=env.observation_space.n\n",
    "outputdim=env.action_space.n\n",
    "hiddendim=100\n",
    "print(inputdim, outputdim)\n",
    "X=tf.placeholder(tf.float32, shape = [None,inputdim],name=\"X\")\n",
    "Y=tf.placeholder(tf.float32, shape = [None,outputdim],name=\"Y\")\n",
    "\n",
    "#weight1=tf.get_variable(\"w1\", initializer=tf.contrib.layers.xavier_initializer(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "#bias1=tf.get_variable(\"b1\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight2=tf.get_variable(\"w2\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias2=tf.get_variable(\"b2\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight3=tf.get_variable(\"w3\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias3=tf.get_variable(\"b3\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight4=tf.get_variable(\"w4\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "#bias4=tf.get_variable(\"b4\", initializer=tf.contrib.layers.xavier_initializer(), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#weight1=tf.get_variable(\"w1\", initializer=tf.initializers.random_normal(0,0.05), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "#bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight2=tf.get_variable(\"w2\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight3=tf.get_variable(\"w3\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight4=tf.get_variable(\"w4\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "#bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "weight1=tf.get_variable(\"w1\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight2=tf.get_variable(\"w2\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight3=tf.get_variable(\"w3\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight4=tf.get_variable(\"w4\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist=[weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]\n",
    "varlist=[weight1,weight4,bias1,bias4]\n",
    "H1=tf.nn.relu(tf.nn.xw_plus_b(X,weight1,bias1))\n",
    "#H2=tf.nn.relu(tf.nn.xw_plus_b(H1,weight2,bias2))\n",
    "#H3=tf.nn.relu(tf.nn.xw_plus_b(H2,weight3,bias3))\n",
    "#Y_pred=tf.nn.xw_plus_b(H3,weight4,bias4)\n",
    "Y_pred=tf.nn.xw_plus_b(H1,weight4,bias4)\n",
    "\n",
    "\n",
    "loss=tf.reduce_sum(tf.square(Y-Y_pred))\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, step: 1, total_reward: -1.010000, epsilon: 0.099000\n",
      "Episode: 1, step: 2, total_reward: -1.020000, epsilon: 0.098010\n",
      "Episode: 2, step: 1, total_reward: -1.010000, epsilon: 0.097030\n",
      "Episode: 3, step: 1, total_reward: -1.010000, epsilon: 0.096060\n",
      "Episode: 4, step: 3, total_reward: -1.030000, epsilon: 0.095099\n",
      "Episode: 5, step: 1, total_reward: -1.010000, epsilon: 0.094148\n",
      "Episode: 6, step: 1, total_reward: -1.010000, epsilon: 0.093207\n",
      "Episode: 7, step: 1, total_reward: -1.010000, epsilon: 0.092274\n",
      "Episode: 8, step: 1, total_reward: -1.010000, epsilon: 0.091352\n",
      "Episode: 9, step: 2, total_reward: -1.020000, epsilon: 0.090438\n",
      "Episode: 10, step: 1, total_reward: -1.010000, epsilon: 0.089534\n",
      "Episode: 11, step: 2, total_reward: -1.020000, epsilon: 0.088638\n",
      "Episode: 12, step: 3, total_reward: -1.030000, epsilon: 0.087752\n",
      "Episode: 13, step: 1, total_reward: -1.010000, epsilon: 0.086875\n",
      "Episode: 14, step: 1, total_reward: -1.010000, epsilon: 0.086006\n",
      "Episode: 15, step: 64, total_reward: -1.640000, epsilon: 0.085146\n",
      "Episode: 16, step: 90, total_reward: -1.900000, epsilon: 0.084294\n",
      "Episode: 17, step: 13, total_reward: -1.130000, epsilon: 0.083451\n",
      "Episode: 18, step: 34, total_reward: -1.340000, epsilon: 0.082617\n",
      "Episode: 19, step: 7, total_reward: -1.070000, epsilon: 0.081791\n",
      "Episode: 20, step: 1, total_reward: -1.010000, epsilon: 0.080973\n",
      "Episode: 21, step: 93, total_reward: -1.930000, epsilon: 0.080163\n",
      "Episode: 22, step: 35, total_reward: -1.350000, epsilon: 0.079361\n",
      "Episode: 23, step: 27, total_reward: -1.270000, epsilon: 0.078568\n",
      "Episode: 24, step: 49, total_reward: -1.490000, epsilon: 0.077782\n",
      "Episode: 25, step: 5, total_reward: -1.050000, epsilon: 0.077004\n",
      "Episode: 26, step: 17, total_reward: -1.170000, epsilon: 0.076234\n",
      "Episode: 27, step: 25, total_reward: -1.250000, epsilon: 0.075472\n",
      "Episode: 28, step: 4, total_reward: -1.040000, epsilon: 0.074717\n",
      "Episode: 29, step: 99, total_reward: -1.000000, epsilon: 0.073970\n",
      "Episode: 30, step: 38, total_reward: -1.380000, epsilon: 0.073230\n",
      "Episode: 31, step: 7, total_reward: -1.070000, epsilon: 0.072498\n",
      "Episode: 32, step: 47, total_reward: -1.470000, epsilon: 0.071773\n",
      "Episode: 33, step: 99, total_reward: -1.000000, epsilon: 0.071055\n",
      "Episode: 34, step: 99, total_reward: -1.000000, epsilon: 0.070345\n",
      "Episode: 35, step: 3, total_reward: -1.030000, epsilon: 0.069641\n",
      "Episode: 36, step: 73, total_reward: -1.730000, epsilon: 0.068945\n",
      "Episode: 37, step: 84, total_reward: -1.840000, epsilon: 0.068255\n",
      "Episode: 38, step: 99, total_reward: -1.000000, epsilon: 0.067573\n",
      "Episode: 39, step: 40, total_reward: 0.600000, epsilon: 0.066897\n",
      "[0, 1, 2, 6, 10, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 9, 8, 4, 0, 1, 2, 6, 10, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15]\n",
      "Episode: 40, step: 5, total_reward: -1.050000, epsilon: 0.066228\n",
      "Episode: 41, step: 5, total_reward: 0.950000, epsilon: 0.065566\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 42, step: 5, total_reward: 0.950000, epsilon: 0.064910\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 43, step: 5, total_reward: 0.950000, epsilon: 0.064261\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 44, step: 5, total_reward: 0.950000, epsilon: 0.063619\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 45, step: 5, total_reward: 0.950000, epsilon: 0.062982\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 46, step: 5, total_reward: 0.950000, epsilon: 0.062353\n",
      "[0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 47, step: 3, total_reward: -1.030000, epsilon: 0.061729\n",
      "Episode: 48, step: 7, total_reward: 0.930000, epsilon: 0.061112\n",
      "[0, 4, 8, 9, 13, 9, 13, 14, 15]\n",
      "Episode: 49, step: 4, total_reward: -1.040000, epsilon: 0.060501\n",
      "Episode: 50, step: 5, total_reward: 0.950000, epsilon: 0.059896\n",
      "[0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 51, step: 5, total_reward: 0.950000, epsilon: 0.059297\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 52, step: 5, total_reward: 0.950000, epsilon: 0.058704\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 53, step: 5, total_reward: 0.950000, epsilon: 0.058117\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 54, step: 5, total_reward: 0.950000, epsilon: 0.057535\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 55, step: 5, total_reward: 0.950000, epsilon: 0.056960\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 56, step: 5, total_reward: 0.950000, epsilon: 0.056391\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 57, step: 1, total_reward: -1.010000, epsilon: 0.055827\n",
      "Episode: 58, step: 1, total_reward: -1.010000, epsilon: 0.055268\n",
      "Episode: 59, step: 5, total_reward: 0.950000, epsilon: 0.054716\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 60, step: 7, total_reward: 0.930000, epsilon: 0.054169\n",
      "[0, 1, 2, 6, 10, 14, 10, 14, 15]\n",
      "Episode: 61, step: 5, total_reward: 0.950000, epsilon: 0.053627\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 62, step: 5, total_reward: 0.950000, epsilon: 0.053091\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 63, step: 5, total_reward: 0.950000, epsilon: 0.052560\n",
      "[0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 64, step: 5, total_reward: 0.950000, epsilon: 0.052034\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 65, step: 5, total_reward: 0.950000, epsilon: 0.051514\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 66, step: 5, total_reward: 0.950000, epsilon: 0.050999\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 67, step: 5, total_reward: 0.950000, epsilon: 0.050489\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 68, step: 3, total_reward: -1.030000, epsilon: 0.049984\n",
      "Episode: 69, step: 6, total_reward: 0.940000, epsilon: 0.049484\n",
      "[0, 1, 2, 2, 6, 10, 14, 15]\n",
      "Episode: 70, step: 5, total_reward: 0.950000, epsilon: 0.048989\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 71, step: 7, total_reward: 0.930000, epsilon: 0.048499\n",
      "[0, 1, 2, 6, 10, 14, 10, 14, 15]\n",
      "Episode: 72, step: 5, total_reward: 0.950000, epsilon: 0.048014\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 73, step: 5, total_reward: 0.950000, epsilon: 0.047534\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 74, step: 5, total_reward: 0.950000, epsilon: 0.047059\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 75, step: 6, total_reward: 0.940000, epsilon: 0.046588\n",
      "[0, 1, 2, 6, 10, 14, 14, 15]\n",
      "Episode: 76, step: 5, total_reward: 0.950000, epsilon: 0.046122\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 77, step: 4, total_reward: -1.040000, epsilon: 0.045661\n",
      "Episode: 78, step: 6, total_reward: 0.940000, epsilon: 0.045204\n",
      "[0, 0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 79, step: 5, total_reward: 0.950000, epsilon: 0.044752\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 80, step: 5, total_reward: 0.950000, epsilon: 0.044305\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 81, step: 5, total_reward: 0.950000, epsilon: 0.043862\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 82, step: 5, total_reward: 0.950000, epsilon: 0.043423\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 83, step: 1, total_reward: -1.010000, epsilon: 0.042989\n",
      "Episode: 84, step: 5, total_reward: 0.950000, epsilon: 0.042559\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 85, step: 5, total_reward: 0.950000, epsilon: 0.042133\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 86, step: 5, total_reward: 0.950000, epsilon: 0.041712\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 87, step: 5, total_reward: 0.950000, epsilon: 0.041295\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 88, step: 5, total_reward: 0.950000, epsilon: 0.040882\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 89, step: 5, total_reward: 0.950000, epsilon: 0.040473\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 90, step: 4, total_reward: -1.040000, epsilon: 0.040068\n",
      "Episode: 91, step: 5, total_reward: 0.950000, epsilon: 0.039668\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 92, step: 6, total_reward: 0.940000, epsilon: 0.039271\n",
      "[0, 1, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 93, step: 5, total_reward: 0.950000, epsilon: 0.038878\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 94, step: 5, total_reward: 0.950000, epsilon: 0.038490\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 95, step: 5, total_reward: 0.950000, epsilon: 0.038105\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 96, step: 5, total_reward: 0.950000, epsilon: 0.037724\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 97, step: 5, total_reward: 0.950000, epsilon: 0.037346\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 98, step: 5, total_reward: 0.950000, epsilon: 0.036973\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 99, step: 5, total_reward: 0.950000, epsilon: 0.036603\n",
      "[0, 1, 2, 6, 10, 14, 15]\n",
      "min step:  5 min step episode 41\n",
      "shortest path:  [0, 1, 2, 6, 10, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "sess.run(init)\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "\n",
    "    for step in range(0,max_step):\n",
    "        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m[0][state]=1.0\n",
    "        Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "        if np.random.rand()>epsilon :\n",
    "            action=np.argmax(Q_pred[0])\n",
    "        else:\n",
    "            action=np.random.choice(env.action_space.n)\n",
    "        \n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "        \n",
    "        if reward==0 : \n",
    "            reward=-0.01\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1.0\n",
    "            \n",
    "        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m_next[0][state_next]=1.0\n",
    "        Q_pred_next=sess.run(Y_pred, feed_dict={X:state_m_next}) \n",
    "\n",
    "        if np.random.rand()>epsilon :\n",
    "            action_next=np.argmax(Q_pred_next[0])\n",
    "        else:\n",
    "            action_next=np.random.choice(env.action_space.n)\n",
    "\n",
    "        Y_true=copy.deepcopy(Q_pred[0])\n",
    "\n",
    "        if reward==1.0 and game_over==1:\n",
    "            Y_true[action]= reward\n",
    "        else :\n",
    "            Y_true[action]=reward+gamma* (Q_pred_next[0][action_next])\n",
    "        Y_true=np.reshape(Y_true,[1,env.action_space.n])\n",
    "        for j in range(0,1):\n",
    "            sess.run([opt],feed_dict={X:state_m,Y:Y_true})\n",
    "            cost= sess.run([loss],feed_dict={X:state_m,Y:Y_true})\n",
    "#            print(cost)\n",
    "            \n",
    "        \n",
    "        \n",
    "#        print(state, Q_pred,Y_true,action)\n",
    "#        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "#        state_m[0][state]=1.0\n",
    "#        Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "#        cost= sess.run([loss],feed_dict={X:state_m,Y:Y_true})\n",
    "#        print(Q_pred,cost)\n",
    "        state=state_next\n",
    "\n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "\n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.04214111  0.7884078  -0.19296977 -0.20802286]\n",
      " [ 0.02253047 -1.7311689  -0.10026854 -1.2524462 ]\n",
      " [-0.00903405  0.77869    -0.84523964 -0.02939156]\n",
      " [-0.5327989  -0.92588234 -1.3497949  -1.02047   ]\n",
      " [-0.03079759  0.8057017  -0.18424651  0.28799987]\n",
      " [ 0.39813387 -0.11779952  0.12235084 -0.12064756]\n",
      " [-0.32493606  0.8641199  -1.3721756  -0.6321842 ]\n",
      " [ 0.30663833  0.7602915  -0.30635455 -0.5011697 ]\n",
      " [ 0.7878032   0.26903173  0.9409461   0.12362932]\n",
      " [ 0.3653009   1.0195056  -0.07024732 -0.8605039 ]\n",
      " [ 0.3462413   1.4228714  -0.7153479  -0.743021  ]\n",
      " [ 0.14880432  0.5451438  -0.04748243 -0.21457759]\n",
      " [ 0.28126222  0.55666614  0.38701117 -0.14305371]\n",
      " [ 0.06552593  0.54800725  1.0287858   0.17923984]\n",
      " [ 0.76084495  0.44646177  1.0206147   0.20011187]\n",
      " [ 0.14337471  0.4271248   0.18207635 -0.14246172]]\n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([16,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,15):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "print(Q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
