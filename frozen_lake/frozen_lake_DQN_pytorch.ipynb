{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "#env =FrozenLakeEnv(is_slippery=False,map_name=\"8x8\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,para,bias=True):\n",
    "        super(Net,self).__init__()\n",
    "        self.input_dim=para['input_dim']\n",
    "        self.output_dim=para['output_dim']\n",
    "        self.hidden_dim=para['hidden_dim']\n",
    "        \n",
    "        self.w1=Parameter(torch.FloatTensor(self.input_dim,self.hidden_dim))\n",
    "        self.b1=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w1)\n",
    "#        nn.init.kaiming_uniform_(self.w1)\n",
    "        nn.init.uniform_(self.b1,-0.001,0.001)\n",
    "\n",
    "        self.w2=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b2=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w2)\n",
    "#        nn.init.kaiming_uniform_(self.w2)\n",
    "        nn.init.uniform_(self.b2,-0.001,0.001)\n",
    "        \n",
    "        self.w3=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b3=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w3)\n",
    "#        nn.init.kaiming_uniform_(self.w3)\n",
    "        nn.init.uniform_(self.b3,-0.001,0.001)\n",
    "        \n",
    "        self.w4=Parameter(torch.FloatTensor(self.hidden_dim,self.output_dim))\n",
    "        self.b4=Parameter(torch.FloatTensor(self.output_dim))\n",
    "        nn.init.kaiming_normal_(self.w4)\n",
    "#        nn.init.kaiming_uniform_(self.w4)\n",
    "        nn.init.uniform_(self.b4,-0.001,0.001)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        H=torch.relu( torch.einsum('li,ij->lj',X,self.w1)+self.b1)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w2)+self.b2)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w3)+self.b3)\n",
    "        Y=torch.einsum('li,ij->lj',H,self.w4)+self.b4\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def copy_val(self,Net2):\n",
    "        self.w1.data.copy_(Net2.w1)\n",
    "        self.w2.data.copy_(Net2.w2)\n",
    "        self.w3.data.copy_(Net2.w3)\n",
    "        self.w4.data.copy_(Net2.w4)\n",
    "        self.b1.data.copy_(Net2.b1)\n",
    "        self.b2.data.copy_(Net2.b2)\n",
    "        self.b3.data.copy_(Net2.b3)\n",
    "        self.b4.data.copy_(Net2.b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=env.observation_space.n\n",
    "output_dim=env.action_space.n\n",
    "hidden_dim=100\n",
    "lr=0.001\n",
    "lr2=1.0\n",
    "\n",
    "para={'input_dim': input_dim, 'output_dim': output_dim, 'hidden_dim': hidden_dim}\n",
    "model=Net(para)\n",
    "model0=Net(para)\n",
    "model0.copy_val(model)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, step: 6, total_reward: -1.050000, epsilon: 0.990000\n",
      "Episode: 1, step: 15, total_reward: -1.140000, epsilon: 0.980100\n",
      "Episode: 2, step: 10, total_reward: -1.090000, epsilon: 0.970299\n",
      "Episode: 3, step: 8, total_reward: -1.070000, epsilon: 0.960596\n",
      "Episode: 4, step: 2, total_reward: -1.010000, epsilon: 0.950990\n",
      "Episode: 5, step: 17, total_reward: -1.160000, epsilon: 0.941480\n",
      "Episode: 6, step: 5, total_reward: -1.040000, epsilon: 0.932065\n",
      "Episode: 7, step: 6, total_reward: -1.050000, epsilon: 0.922745\n",
      "Episode: 8, step: 18, total_reward: -1.170000, epsilon: 0.913517\n",
      "Episode: 9, step: 4, total_reward: -1.030000, epsilon: 0.904382\n",
      "Episode: 10, step: 3, total_reward: -1.020000, epsilon: 0.895338\n",
      "Episode: 11, step: 3, total_reward: -1.020000, epsilon: 0.886385\n",
      "Episode: 12, step: 9, total_reward: -1.080000, epsilon: 0.877521\n",
      "Episode: 13, step: 34, total_reward: -1.330000, epsilon: 0.868746\n",
      "Episode: 14, step: 2, total_reward: -1.010000, epsilon: 0.860058\n",
      "Episode: 15, step: 11, total_reward: -1.100000, epsilon: 0.851458\n",
      "Episode: 16, step: 4, total_reward: -1.030000, epsilon: 0.842943\n",
      "Episode: 17, step: 2, total_reward: -1.010000, epsilon: 0.834514\n",
      "Episode: 18, step: 12, total_reward: -1.110000, epsilon: 0.826169\n",
      "Episode: 19, step: 3, total_reward: -1.020000, epsilon: 0.817907\n",
      "Episode: 20, step: 11, total_reward: -1.100000, epsilon: 0.809728\n",
      "Episode: 21, step: 4, total_reward: -1.030000, epsilon: 0.801631\n",
      "Episode: 22, step: 8, total_reward: 0.930000, epsilon: 0.793614\n",
      "[0, 4, 4, 8, 9, 10, 14, 14, 15]\n",
      "Episode: 23, step: 25, total_reward: -1.240000, epsilon: 0.785678\n",
      "Episode: 24, step: 14, total_reward: -1.130000, epsilon: 0.777821\n",
      "Episode: 25, step: 7, total_reward: -1.060000, epsilon: 0.770043\n",
      "Episode: 26, step: 7, total_reward: -1.060000, epsilon: 0.762343\n",
      "Episode: 27, step: 6, total_reward: -1.050000, epsilon: 0.754719\n",
      "Episode: 28, step: 11, total_reward: -1.100000, epsilon: 0.747172\n",
      "Episode: 29, step: 6, total_reward: -1.050000, epsilon: 0.739700\n",
      "Episode: 30, step: 2, total_reward: -1.010000, epsilon: 0.732303\n",
      "Episode: 31, step: 10, total_reward: -1.090000, epsilon: 0.724980\n",
      "Episode: 32, step: 4, total_reward: -1.030000, epsilon: 0.717731\n",
      "Episode: 33, step: 21, total_reward: -1.200000, epsilon: 0.710553\n",
      "Episode: 34, step: 8, total_reward: -1.070000, epsilon: 0.703448\n",
      "Episode: 35, step: 12, total_reward: -1.110000, epsilon: 0.696413\n",
      "Episode: 36, step: 4, total_reward: -1.030000, epsilon: 0.689449\n",
      "Episode: 37, step: 2, total_reward: -1.010000, epsilon: 0.682555\n",
      "Episode: 38, step: 5, total_reward: -1.040000, epsilon: 0.675729\n",
      "Episode: 39, step: 15, total_reward: -1.140000, epsilon: 0.668972\n",
      "Episode: 40, step: 6, total_reward: -1.050000, epsilon: 0.662282\n",
      "Episode: 41, step: 8, total_reward: -1.070000, epsilon: 0.655659\n",
      "Episode: 42, step: 13, total_reward: -1.120000, epsilon: 0.649103\n",
      "Episode: 43, step: 2, total_reward: -1.010000, epsilon: 0.642612\n",
      "Episode: 44, step: 16, total_reward: -1.150000, epsilon: 0.636185\n",
      "Episode: 45, step: 15, total_reward: -1.140000, epsilon: 0.629824\n",
      "Episode: 46, step: 29, total_reward: -1.280000, epsilon: 0.623525\n",
      "Episode: 47, step: 3, total_reward: -1.020000, epsilon: 0.617290\n",
      "Episode: 48, step: 17, total_reward: -1.160000, epsilon: 0.611117\n",
      "Episode: 49, step: 16, total_reward: -1.150000, epsilon: 0.605006\n",
      "Episode: 50, step: 25, total_reward: -1.240000, epsilon: 0.598956\n",
      "Episode: 51, step: 6, total_reward: -1.050000, epsilon: 0.592966\n",
      "Episode: 52, step: 5, total_reward: -1.040000, epsilon: 0.587037\n",
      "Episode: 53, step: 14, total_reward: -1.130000, epsilon: 0.581166\n",
      "Episode: 54, step: 4, total_reward: -1.030000, epsilon: 0.575355\n",
      "Episode: 55, step: 2, total_reward: -1.010000, epsilon: 0.569601\n",
      "Episode: 56, step: 12, total_reward: -1.110000, epsilon: 0.563905\n",
      "Episode: 57, step: 6, total_reward: -1.050000, epsilon: 0.558266\n",
      "Episode: 58, step: 17, total_reward: -1.160000, epsilon: 0.552683\n",
      "Episode: 59, step: 22, total_reward: -1.210000, epsilon: 0.547157\n",
      "Episode: 60, step: 7, total_reward: -1.060000, epsilon: 0.541685\n",
      "Episode: 61, step: 7, total_reward: -1.060000, epsilon: 0.536268\n",
      "Episode: 62, step: 91, total_reward: -1.900000, epsilon: 0.530906\n",
      "Episode: 63, step: 3, total_reward: -1.020000, epsilon: 0.525596\n",
      "Episode: 64, step: 9, total_reward: -1.080000, epsilon: 0.520341\n",
      "Episode: 65, step: 61, total_reward: -1.600000, epsilon: 0.515137\n",
      "Episode: 66, step: 6, total_reward: -1.050000, epsilon: 0.509986\n",
      "Episode: 67, step: 20, total_reward: 0.810000, epsilon: 0.504886\n",
      "[0, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 9, 13, 14, 10, 9, 13, 14, 13, 14, 15]\n",
      "Episode: 68, step: 16, total_reward: -1.150000, epsilon: 0.499837\n",
      "Episode: 69, step: 10, total_reward: -1.090000, epsilon: 0.494839\n",
      "Episode: 70, step: 13, total_reward: -1.120000, epsilon: 0.489890\n",
      "Episode: 71, step: 18, total_reward: -1.170000, epsilon: 0.484991\n",
      "Episode: 72, step: 20, total_reward: -1.190000, epsilon: 0.480141\n",
      "Episode: 73, step: 5, total_reward: -1.040000, epsilon: 0.475340\n",
      "Episode: 74, step: 31, total_reward: -1.300000, epsilon: 0.470587\n",
      "Episode: 75, step: 17, total_reward: -1.160000, epsilon: 0.465881\n",
      "Episode: 76, step: 9, total_reward: -1.080000, epsilon: 0.461222\n",
      "Episode: 77, step: 22, total_reward: -1.210000, epsilon: 0.456610\n",
      "Episode: 78, step: 3, total_reward: -1.020000, epsilon: 0.452044\n",
      "Episode: 79, step: 28, total_reward: 0.730000, epsilon: 0.447523\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 4, 8, 8, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 80, step: 19, total_reward: 0.820000, epsilon: 0.443048\n",
      "[0, 0, 4, 8, 4, 8, 4, 4, 4, 4, 8, 4, 4, 8, 9, 13, 13, 14, 14, 15]\n",
      "Episode: 81, step: 7, total_reward: -1.060000, epsilon: 0.438618\n",
      "Episode: 82, step: 25, total_reward: -1.240000, epsilon: 0.434231\n",
      "Episode: 83, step: 61, total_reward: 0.400000, epsilon: 0.429889\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 8, 8, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 8, 4, 8, 8, 9, 13, 14, 14, 15]\n",
      "Episode: 84, step: 9, total_reward: -1.080000, epsilon: 0.425590\n",
      "Episode: 85, step: 25, total_reward: -1.240000, epsilon: 0.421334\n",
      "Episode: 86, step: 11, total_reward: -1.100000, epsilon: 0.417121\n",
      "Episode: 87, step: 3, total_reward: -1.020000, epsilon: 0.412950\n",
      "Episode: 88, step: 41, total_reward: -1.400000, epsilon: 0.408820\n",
      "Episode: 89, step: 16, total_reward: -1.150000, epsilon: 0.404732\n",
      "Episode: 90, step: 5, total_reward: -1.040000, epsilon: 0.400685\n",
      "Episode: 91, step: 14, total_reward: 0.870000, epsilon: 0.396678\n",
      "[0, 0, 4, 8, 8, 4, 8, 9, 10, 14, 10, 14, 10, 14, 15]\n",
      "Episode: 92, step: 15, total_reward: -1.140000, epsilon: 0.392711\n",
      "Episode: 93, step: 7, total_reward: -1.060000, epsilon: 0.388784\n",
      "Episode: 94, step: 6, total_reward: -1.050000, epsilon: 0.384896\n",
      "Episode: 95, step: 5, total_reward: -1.040000, epsilon: 0.381047\n",
      "Episode: 96, step: 11, total_reward: -1.100000, epsilon: 0.377237\n",
      "Episode: 97, step: 6, total_reward: -1.050000, epsilon: 0.373464\n",
      "Episode: 98, step: 19, total_reward: -1.180000, epsilon: 0.369730\n",
      "Episode: 99, step: 18, total_reward: 0.830000, epsilon: 0.366032\n",
      "[0, 1, 0, 0, 1, 0, 1, 2, 3, 3, 2, 2, 6, 10, 14, 14, 10, 14, 15]\n",
      "Episode: 100, step: 23, total_reward: -1.220000, epsilon: 0.362372\n",
      "Episode: 101, step: 12, total_reward: -1.110000, epsilon: 0.358748\n",
      "Episode: 102, step: 13, total_reward: -1.120000, epsilon: 0.355161\n",
      "Episode: 103, step: 15, total_reward: -1.140000, epsilon: 0.351609\n",
      "Episode: 104, step: 23, total_reward: -1.220000, epsilon: 0.348093\n",
      "Episode: 105, step: 7, total_reward: -1.060000, epsilon: 0.344612\n",
      "Episode: 106, step: 4, total_reward: -1.030000, epsilon: 0.341166\n",
      "Episode: 107, step: 63, total_reward: -1.620000, epsilon: 0.337754\n",
      "Episode: 108, step: 6, total_reward: -1.050000, epsilon: 0.334377\n",
      "Episode: 109, step: 10, total_reward: -1.090000, epsilon: 0.331033\n",
      "Episode: 110, step: 15, total_reward: -1.140000, epsilon: 0.327723\n",
      "Episode: 111, step: 35, total_reward: -1.340000, epsilon: 0.324446\n",
      "Episode: 112, step: 15, total_reward: -1.140000, epsilon: 0.321201\n",
      "Episode: 113, step: 4, total_reward: -1.030000, epsilon: 0.317989\n",
      "Episode: 114, step: 8, total_reward: -1.070000, epsilon: 0.314809\n",
      "Episode: 115, step: 27, total_reward: -1.260000, epsilon: 0.311661\n",
      "Episode: 116, step: 77, total_reward: -1.760000, epsilon: 0.308544\n",
      "Episode: 117, step: 12, total_reward: -1.110000, epsilon: 0.305459\n",
      "Episode: 118, step: 46, total_reward: -1.450000, epsilon: 0.302404\n",
      "Episode: 119, step: 46, total_reward: -1.450000, epsilon: 0.299380\n",
      "Episode: 120, step: 3, total_reward: -1.020000, epsilon: 0.296387\n",
      "Episode: 121, step: 21, total_reward: -1.200000, epsilon: 0.293423\n",
      "Episode: 122, step: 23, total_reward: -1.220000, epsilon: 0.290488\n",
      "Episode: 123, step: 33, total_reward: -1.320000, epsilon: 0.287584\n",
      "Episode: 124, step: 18, total_reward: 0.830000, epsilon: 0.284708\n",
      "[0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 125, step: 34, total_reward: -1.330000, epsilon: 0.281861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 126, step: 19, total_reward: 0.820000, epsilon: 0.279042\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 8, 9, 13, 14, 13, 14, 15]\n",
      "Episode: 127, step: 37, total_reward: -1.360000, epsilon: 0.276252\n",
      "Episode: 128, step: 63, total_reward: -1.620000, epsilon: 0.273489\n",
      "Episode: 129, step: 21, total_reward: -1.200000, epsilon: 0.270754\n",
      "Episode: 130, step: 28, total_reward: -1.270000, epsilon: 0.268047\n",
      "Episode: 131, step: 20, total_reward: -1.190000, epsilon: 0.265366\n",
      "Episode: 132, step: 46, total_reward: -1.450000, epsilon: 0.262713\n",
      "Episode: 133, step: 6, total_reward: -1.050000, epsilon: 0.260085\n",
      "Episode: 134, step: 66, total_reward: -1.650000, epsilon: 0.257485\n",
      "Episode: 135, step: 52, total_reward: -1.510000, epsilon: 0.254910\n",
      "Episode: 136, step: 2, total_reward: -1.010000, epsilon: 0.252361\n",
      "Episode: 137, step: 21, total_reward: -1.200000, epsilon: 0.249837\n",
      "Episode: 138, step: 31, total_reward: -1.300000, epsilon: 0.247339\n",
      "Episode: 139, step: 50, total_reward: 0.510000, epsilon: 0.244865\n",
      "[0, 1, 2, 3, 2, 2, 3, 2, 3, 2, 1, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1, 2, 6, 2, 3, 2, 3, 2, 3, 2, 3, 2, 6, 10, 14, 15]\n",
      "Episode: 140, step: 23, total_reward: -1.220000, epsilon: 0.242417\n",
      "Episode: 141, step: 33, total_reward: -1.320000, epsilon: 0.239992\n",
      "Episode: 142, step: 4, total_reward: -1.030000, epsilon: 0.237593\n",
      "Episode: 143, step: 16, total_reward: 0.850000, epsilon: 0.235217\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 144, step: 20, total_reward: -1.190000, epsilon: 0.232864\n",
      "Episode: 145, step: 22, total_reward: -1.210000, epsilon: 0.230536\n",
      "Episode: 146, step: 27, total_reward: -1.260000, epsilon: 0.228230\n",
      "Episode: 147, step: 36, total_reward: -1.350000, epsilon: 0.225948\n",
      "Episode: 148, step: 84, total_reward: 0.170000, epsilon: 0.223689\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 149, step: 22, total_reward: -1.210000, epsilon: 0.221452\n",
      "Episode: 150, step: 35, total_reward: -1.340000, epsilon: 0.219237\n",
      "Episode: 151, step: 26, total_reward: -1.250000, epsilon: 0.217045\n",
      "Episode: 152, step: 19, total_reward: -1.180000, epsilon: 0.214874\n",
      "Episode: 153, step: 17, total_reward: -1.160000, epsilon: 0.212726\n",
      "Episode: 154, step: 31, total_reward: -1.300000, epsilon: 0.210598\n",
      "Episode: 155, step: 23, total_reward: -1.220000, epsilon: 0.208492\n",
      "Episode: 156, step: 100, total_reward: -1.000000, epsilon: 0.206408\n",
      "Episode: 157, step: 12, total_reward: -1.110000, epsilon: 0.204343\n",
      "Episode: 158, step: 31, total_reward: 0.700000, epsilon: 0.202300\n",
      "[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 159, step: 6, total_reward: -1.050000, epsilon: 0.200277\n",
      "Episode: 160, step: 33, total_reward: -1.320000, epsilon: 0.198274\n",
      "Episode: 161, step: 27, total_reward: -1.260000, epsilon: 0.196292\n",
      "Episode: 162, step: 43, total_reward: -1.420000, epsilon: 0.194329\n",
      "Episode: 163, step: 38, total_reward: -1.370000, epsilon: 0.192385\n",
      "Episode: 164, step: 6, total_reward: -1.050000, epsilon: 0.190461\n",
      "Episode: 165, step: 16, total_reward: -1.150000, epsilon: 0.188557\n",
      "Episode: 166, step: 71, total_reward: -1.700000, epsilon: 0.186671\n",
      "Episode: 167, step: 100, total_reward: -1.000000, epsilon: 0.184805\n",
      "Episode: 168, step: 58, total_reward: -1.570000, epsilon: 0.182957\n",
      "Episode: 169, step: 56, total_reward: -1.550000, epsilon: 0.181127\n",
      "Episode: 170, step: 17, total_reward: 0.840000, epsilon: 0.179316\n",
      "[0, 0, 0, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 171, step: 19, total_reward: 0.820000, epsilon: 0.177523\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 8, 9, 13, 14, 13, 14, 15]\n",
      "Episode: 172, step: 22, total_reward: 0.790000, epsilon: 0.175747\n",
      "[0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 173, step: 12, total_reward: -1.110000, epsilon: 0.173990\n",
      "Episode: 174, step: 50, total_reward: -1.490000, epsilon: 0.172250\n",
      "Episode: 175, step: 25, total_reward: -1.240000, epsilon: 0.170527\n",
      "Episode: 176, step: 52, total_reward: -1.510000, epsilon: 0.168822\n",
      "Episode: 177, step: 94, total_reward: -1.930000, epsilon: 0.167134\n",
      "Episode: 178, step: 42, total_reward: -1.410000, epsilon: 0.165463\n",
      "Episode: 179, step: 65, total_reward: -1.640000, epsilon: 0.163808\n",
      "Episode: 180, step: 63, total_reward: -1.620000, epsilon: 0.162170\n",
      "Episode: 181, step: 23, total_reward: -1.220000, epsilon: 0.160548\n",
      "Episode: 182, step: 11, total_reward: -1.100000, epsilon: 0.158943\n",
      "Episode: 183, step: 13, total_reward: -1.120000, epsilon: 0.157353\n",
      "Episode: 184, step: 46, total_reward: -1.450000, epsilon: 0.155780\n",
      "Episode: 185, step: 43, total_reward: -1.420000, epsilon: 0.154222\n",
      "Episode: 186, step: 48, total_reward: -1.470000, epsilon: 0.152680\n",
      "Episode: 187, step: 100, total_reward: -1.000000, epsilon: 0.151153\n",
      "Episode: 188, step: 100, total_reward: -1.000000, epsilon: 0.149641\n",
      "Episode: 189, step: 44, total_reward: -1.430000, epsilon: 0.148145\n",
      "Episode: 190, step: 64, total_reward: -1.630000, epsilon: 0.146664\n",
      "Episode: 191, step: 27, total_reward: -1.260000, epsilon: 0.145197\n",
      "Episode: 192, step: 26, total_reward: -1.250000, epsilon: 0.143745\n",
      "Episode: 193, step: 58, total_reward: -1.570000, epsilon: 0.142307\n",
      "Episode: 194, step: 13, total_reward: -1.120000, epsilon: 0.140884\n",
      "Episode: 195, step: 39, total_reward: -1.380000, epsilon: 0.139476\n",
      "Episode: 196, step: 73, total_reward: -1.720000, epsilon: 0.138081\n",
      "Episode: 197, step: 96, total_reward: -1.950000, epsilon: 0.136700\n",
      "Episode: 198, step: 12, total_reward: -1.110000, epsilon: 0.135333\n",
      "Episode: 199, step: 27, total_reward: -1.260000, epsilon: 0.133980\n",
      "Episode: 200, step: 6, total_reward: -1.050000, epsilon: 0.132640\n",
      "Episode: 201, step: 31, total_reward: -1.300000, epsilon: 0.131313\n",
      "Episode: 202, step: 12, total_reward: -1.110000, epsilon: 0.130000\n",
      "Episode: 203, step: 35, total_reward: -1.340000, epsilon: 0.128700\n",
      "Episode: 204, step: 6, total_reward: -1.050000, epsilon: 0.127413\n",
      "Episode: 205, step: 13, total_reward: -1.120000, epsilon: 0.126139\n",
      "Episode: 206, step: 6, total_reward: -1.050000, epsilon: 0.124878\n",
      "Episode: 207, step: 22, total_reward: -1.210000, epsilon: 0.123629\n",
      "Episode: 208, step: 23, total_reward: -1.220000, epsilon: 0.122393\n",
      "Episode: 209, step: 100, total_reward: -1.000000, epsilon: 0.121169\n",
      "Episode: 210, step: 10, total_reward: -1.090000, epsilon: 0.119957\n",
      "Episode: 211, step: 16, total_reward: -1.150000, epsilon: 0.118758\n",
      "Episode: 212, step: 4, total_reward: -1.030000, epsilon: 0.117570\n",
      "Episode: 213, step: 33, total_reward: -1.320000, epsilon: 0.116394\n",
      "Episode: 214, step: 8, total_reward: -1.070000, epsilon: 0.115230\n",
      "Episode: 215, step: 40, total_reward: -1.390000, epsilon: 0.114078\n",
      "Episode: 216, step: 22, total_reward: -1.210000, epsilon: 0.112937\n",
      "Episode: 217, step: 100, total_reward: -1.000000, epsilon: 0.111808\n",
      "Episode: 218, step: 34, total_reward: -1.330000, epsilon: 0.110690\n",
      "Episode: 219, step: 29, total_reward: -1.280000, epsilon: 0.109583\n",
      "Episode: 220, step: 57, total_reward: -1.560000, epsilon: 0.108487\n",
      "Episode: 221, step: 100, total_reward: -1.000000, epsilon: 0.107402\n",
      "Episode: 222, step: 47, total_reward: -1.460000, epsilon: 0.106328\n",
      "Episode: 223, step: 59, total_reward: -1.580000, epsilon: 0.105265\n",
      "Episode: 224, step: 39, total_reward: -1.380000, epsilon: 0.104212\n",
      "Episode: 225, step: 61, total_reward: -1.600000, epsilon: 0.103170\n",
      "Episode: 226, step: 74, total_reward: -1.730000, epsilon: 0.102138\n",
      "Episode: 227, step: 100, total_reward: -1.000000, epsilon: 0.101117\n",
      "Episode: 228, step: 84, total_reward: -1.830000, epsilon: 0.100106\n",
      "Episode: 229, step: 99, total_reward: -1.980000, epsilon: 0.099105\n",
      "Episode: 230, step: 51, total_reward: -1.500000, epsilon: 0.098114\n",
      "Episode: 231, step: 56, total_reward: -1.550000, epsilon: 0.097133\n",
      "Episode: 232, step: 55, total_reward: -1.540000, epsilon: 0.096161\n",
      "Episode: 233, step: 34, total_reward: -1.330000, epsilon: 0.095200\n",
      "Episode: 234, step: 100, total_reward: -1.000000, epsilon: 0.094248\n",
      "Episode: 235, step: 33, total_reward: -1.320000, epsilon: 0.093305\n",
      "Episode: 236, step: 26, total_reward: -1.250000, epsilon: 0.092372\n",
      "Episode: 237, step: 23, total_reward: -1.220000, epsilon: 0.091448\n",
      "Episode: 238, step: 13, total_reward: -1.120000, epsilon: 0.090534\n",
      "Episode: 239, step: 10, total_reward: -1.090000, epsilon: 0.089629\n",
      "Episode: 240, step: 57, total_reward: -1.560000, epsilon: 0.088732\n",
      "Episode: 241, step: 100, total_reward: -1.000000, epsilon: 0.087845\n",
      "Episode: 242, step: 91, total_reward: -1.900000, epsilon: 0.086967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 243, step: 33, total_reward: -1.320000, epsilon: 0.086097\n",
      "Episode: 244, step: 100, total_reward: -1.000000, epsilon: 0.085236\n",
      "Episode: 245, step: 9, total_reward: -1.080000, epsilon: 0.084384\n",
      "Episode: 246, step: 32, total_reward: -1.310000, epsilon: 0.083540\n",
      "Episode: 247, step: 50, total_reward: 0.510000, epsilon: 0.082704\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 248, step: 24, total_reward: -1.230000, epsilon: 0.081877\n",
      "Episode: 249, step: 100, total_reward: -1.000000, epsilon: 0.081059\n",
      "Episode: 250, step: 54, total_reward: -1.530000, epsilon: 0.080248\n",
      "Episode: 251, step: 70, total_reward: -1.690000, epsilon: 0.079445\n",
      "Episode: 252, step: 23, total_reward: -1.220000, epsilon: 0.078651\n",
      "Episode: 253, step: 100, total_reward: -1.000000, epsilon: 0.077864\n",
      "Episode: 254, step: 25, total_reward: -1.240000, epsilon: 0.077086\n",
      "Episode: 255, step: 13, total_reward: -1.120000, epsilon: 0.076315\n",
      "Episode: 256, step: 37, total_reward: -1.360000, epsilon: 0.075552\n",
      "Episode: 257, step: 100, total_reward: -1.000000, epsilon: 0.074796\n",
      "Episode: 258, step: 46, total_reward: -1.450000, epsilon: 0.074048\n",
      "Episode: 259, step: 100, total_reward: -1.000000, epsilon: 0.073308\n",
      "Episode: 260, step: 100, total_reward: -1.000000, epsilon: 0.072575\n",
      "Episode: 261, step: 100, total_reward: -1.000000, epsilon: 0.071849\n",
      "Episode: 262, step: 100, total_reward: -1.000000, epsilon: 0.071131\n",
      "Episode: 263, step: 100, total_reward: -1.000000, epsilon: 0.070419\n",
      "Episode: 264, step: 100, total_reward: -1.000000, epsilon: 0.069715\n",
      "Episode: 265, step: 26, total_reward: -1.250000, epsilon: 0.069018\n",
      "Episode: 266, step: 11, total_reward: -1.100000, epsilon: 0.068328\n",
      "Episode: 267, step: 82, total_reward: -1.810000, epsilon: 0.067644\n",
      "Episode: 268, step: 100, total_reward: -1.000000, epsilon: 0.066968\n",
      "Episode: 269, step: 100, total_reward: -1.000000, epsilon: 0.066298\n",
      "Episode: 270, step: 56, total_reward: -1.550000, epsilon: 0.065635\n",
      "Episode: 271, step: 90, total_reward: -1.890000, epsilon: 0.064979\n",
      "Episode: 272, step: 100, total_reward: -1.000000, epsilon: 0.064329\n",
      "Episode: 273, step: 9, total_reward: -1.080000, epsilon: 0.063686\n",
      "Episode: 274, step: 100, total_reward: -1.000000, epsilon: 0.063049\n",
      "Episode: 275, step: 29, total_reward: -1.280000, epsilon: 0.062419\n",
      "Episode: 276, step: 5, total_reward: -1.040000, epsilon: 0.061794\n",
      "Episode: 277, step: 100, total_reward: -1.000000, epsilon: 0.061176\n",
      "Episode: 278, step: 20, total_reward: -1.190000, epsilon: 0.060565\n",
      "Episode: 279, step: 76, total_reward: -1.750000, epsilon: 0.059959\n",
      "Episode: 280, step: 78, total_reward: -1.770000, epsilon: 0.059359\n",
      "Episode: 281, step: 5, total_reward: -1.040000, epsilon: 0.058766\n",
      "Episode: 282, step: 64, total_reward: 0.370000, epsilon: 0.058178\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1, 2, 3, 2, 6, 10, 14, 15]\n",
      "Episode: 283, step: 100, total_reward: -1.000000, epsilon: 0.057596\n",
      "Episode: 284, step: 100, total_reward: -1.000000, epsilon: 0.057020\n",
      "Episode: 285, step: 89, total_reward: 0.120000, epsilon: 0.056450\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 286, step: 100, total_reward: -1.000000, epsilon: 0.055886\n",
      "Episode: 287, step: 100, total_reward: -1.000000, epsilon: 0.055327\n",
      "Episode: 288, step: 100, total_reward: -1.000000, epsilon: 0.054774\n",
      "Episode: 289, step: 100, total_reward: -1.000000, epsilon: 0.054226\n",
      "Episode: 290, step: 26, total_reward: -1.250000, epsilon: 0.053684\n",
      "Episode: 291, step: 100, total_reward: -1.000000, epsilon: 0.053147\n",
      "Episode: 292, step: 100, total_reward: -1.000000, epsilon: 0.052615\n",
      "Episode: 293, step: 100, total_reward: -1.000000, epsilon: 0.052089\n",
      "Episode: 294, step: 54, total_reward: -1.530000, epsilon: 0.051568\n",
      "Episode: 295, step: 100, total_reward: -1.000000, epsilon: 0.051053\n",
      "Episode: 296, step: 100, total_reward: -1.000000, epsilon: 0.050542\n",
      "Episode: 297, step: 100, total_reward: -1.000000, epsilon: 0.050037\n",
      "Episode: 298, step: 100, total_reward: -1.000000, epsilon: 0.049536\n",
      "Episode: 299, step: 100, total_reward: -1.000000, epsilon: 0.049041\n",
      "min step:  8 min step episode 22\n",
      "shortest path:  [0, 4, 4, 8, 9, 10, 14, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=300\n",
    "max_step=100\n",
    "gamma=0.998\n",
    "epsilon0 = 1.0\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "max_replay=1000\n",
    "replay=deque(maxlen=max_replay)\n",
    "#replay=[]\n",
    "replay0=deque(maxlen=max_replay)\n",
    "replay1=deque(maxlen=max_replay)\n",
    "replay2=deque(maxlen=max_replay)\n",
    "i_step=0\n",
    "batch_size=50\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "    \n",
    "    state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "    state_m[0][state]=1.0\n",
    "    state_m_torch=torch.from_numpy(state_m)\n",
    "    Q_pred=model0(state_m_torch)\n",
    "    Q_pred_np=Q_pred.cpu().detach().numpy()\n",
    "    if np.random.rand()>epsilon :\n",
    "        action=np.argmax(Q_pred_np[0])\n",
    "    else:\n",
    "        action=np.random.choice(env.action_space.n) \n",
    "    for step in range(1,max_step+1):\n",
    "        \n",
    "        i_step+=1\n",
    "   \n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "        \n",
    "        if reward==0 : \n",
    "            reward=-0.01\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1\n",
    "        \n",
    "        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m_next[0][state_next]=1.0\n",
    "        state_m_next_torch=torch.from_numpy(state_m_next)\n",
    "        Q_pred_next=model0(state_m_next_torch)\n",
    "        Q_pred_next=Q_pred_next.cpu().detach().numpy()\n",
    "        if np.random.rand()>epsilon :\n",
    "            action_next=np.argmax(Q_pred_next[0])\n",
    "        else:\n",
    "            action_next=np.random.choice(env.action_space.n)\n",
    "        \n",
    "        #print(Q_pred_np)\n",
    "        Y_true=copy.deepcopy(Q_pred_np[0])\n",
    "        if reward==1.0 and game_over==1:\n",
    "            Y_true[action]= reward\n",
    "        else :\n",
    "            Y_true[action]=reward+gamma* (Q_pred_next[0][action_next])\n",
    "        Y_true=np.reshape(Y_true,[1,env.action_space.n])\n",
    "\n",
    "        replay.append([state_m[0],Y_true])\n",
    "\n",
    "#        if state in replay0:\n",
    "#            index=replay0.index(state)\n",
    "#            replay[index]=[state_m[0],Y_true]\n",
    "#        else:\n",
    "#            replay0.append(state)\n",
    "#            replay.append([state_m[0],Y_true]) \n",
    "        \n",
    "#            game_over=0\n",
    "        # Q-Learning\n",
    "        if i_step%50==0 and i_step!=0 and len(replay)>batch_size:\n",
    "#            print(replay0)\n",
    "            mini_batch = random.sample(replay,batch_size)\n",
    "            states=np.zeros([batch_size,input_dim],dtype=np.float32)\n",
    "            Qs=np.zeros([batch_size,output_dim],dtype=np.float32)\n",
    "            for m in range(batch_size):\n",
    "                states[m]=mini_batch[m][0]\n",
    "                Qs[m]=mini_batch[m][1]\n",
    "            \n",
    "            for i in range(0,1):\n",
    "                optimizer.zero_grad()\n",
    "                states_torch=torch.from_numpy(states)\n",
    "                Q_pred=model(states_torch)\n",
    "                Qs_torch=torch.from_numpy(Qs)\n",
    "                loss=criterion(Q_pred,Qs_torch)\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                cost=loss.data\n",
    "            Q_pred_np=Q_pred.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        if i_step%100==0  :\n",
    "#            print(i_step,\"update\")\n",
    "            model0.copy_val(model)\n",
    "\n",
    "            \n",
    "        state=state_next\n",
    "        state_m=state_m_next\n",
    "        action=action_next\n",
    "        \n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "            \n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n",
    "#print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[-0.23958032 -0.28155452 -0.2315142  -0.2466275 ]\n",
      " [-0.24987632 -0.29821908 -0.19079159 -0.23576882]\n",
      " [-0.2347972   0.32604674 -0.23196468 -0.23123698]\n",
      " [-0.23416203 -0.15894885 -0.10167768 -0.17548004]\n",
      " [-0.21364115 -0.26412454 -0.27650136 -0.2288946 ]\n",
      " [ 0.37619886 -0.19101168 -0.71156687 -0.30705297]\n",
      " [-0.25258195  0.7352591  -0.28643018 -0.25610065]\n",
      " [-0.34214517  0.11717233 -0.12746952 -0.23787361]\n",
      " [-0.22351101 -0.4282443  -0.138744   -0.23075177]\n",
      " [-0.20105517  0.00887756 -0.15171187 -0.26568422]\n",
      " [-0.22746831  0.74186814 -0.21623187 -0.25023845]\n",
      " [-0.1636775   0.20694003  0.21393305 -0.3184836 ]\n",
      " [-0.33215395  0.14660972 -0.2673614  -0.5346207 ]\n",
      " [-0.27952078 -0.26549825  0.39126363 -0.2894014 ]\n",
      " [-0.27806717 -0.20136715  1.0236037  -0.24523228]\n",
      " [-0.4694322   0.30497122  0.23877749  0.068596  ]]\n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([env.observation_space.n,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,env.observation_space.n):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "state_m_torch=torch.from_numpy(state_m)\n",
    "Q_pred=model(state_m_torch)\n",
    "Q_pred=Q_pred.cpu().detach().numpy()\n",
    "print(Q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.rand([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.zeros([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0485, 0.8422, 0.3865, 0.9593, 0.0383, 0.7846, 0.7052, 0.0029, 0.7384,\n",
      "        0.0160]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.copy_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[5]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 2., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=Parameter(torch.FloatTensor(10))\n",
    "w2=Parameter(torch.FloatTensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-5.3691e-26,  4.5588e-41, -5.3998e-26,  4.5588e-41, -5.3680e-26,\n",
      "         4.5588e-41, -5.3685e-26,  4.5588e-41, -5.3689e-26,  4.5588e-41],\n",
      "       requires_grad=True) Parameter containing:\n",
      "tensor([-2.9900e+08,  3.0660e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9900e+08,  3.0660e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.data.copy_(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-2.9900e+08,  3.0660e-41,  0.0000e+00,  0.0000e+00,  6.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       grad_fn=<CopySlices>)\n",
      "Parameter containing:\n",
      "tensor([-2.9900e+08,  3.0660e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w1[4]=6\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=env.observation_space.n\n",
    "output_dim=env.action_space.n\n",
    "hidden_dim=10\n",
    "\n",
    "para={'input_dim': input_dim, 'output_dim': output_dim, 'hidden_dim': hidden_dim}\n",
    "m=Net(para)\n",
    "m2=Net(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3=m2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.copy_val(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clone(): argument 'input' (position 1) must be Tensor, not Net",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-fe29f5b424eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: clone(): argument 'input' (position 1) must be Tensor, not Net"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
