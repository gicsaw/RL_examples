{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "#env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4\n"
     ]
    }
   ],
   "source": [
    "inputdim=env.observation_space.n\n",
    "outputdim=env.action_space.n\n",
    "hiddendim=100\n",
    "print(inputdim, outputdim)\n",
    "X=tf.placeholder(tf.float32, shape = [None,inputdim],name=\"X\")\n",
    "Y=tf.placeholder(tf.float32, shape = [None,outputdim],name=\"Y\")\n",
    "# Y=tf.placeholder(tf.float32,shape=[None,1],name=\"Y\")\n",
    "# action=tf.placeholder(tf.int32,shape=[1],name=\"action\")\n",
    "\n",
    "weight1=tf.get_variable(\"w1\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight2=tf.get_variable(\"w2\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight3=tf.get_variable(\"w3\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight4=tf.get_variable(\"w4\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist=[weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]\n",
    "varlist=[weight1,weight4,bias1,bias4]\n",
    "H1=tf.nn.relu(tf.nn.xw_plus_b(X,weight1,bias1))\n",
    "#H2=tf.nn.relu(tf.nn.xw_plus_b(H1,weight2,bias2))\n",
    "#H3=tf.nn.relu(tf.nn.xw_plus_b(H2,weight3,bias3))\n",
    "#Y_pred=tf.nn.xw_plus_b(H3,weight4,bias4)\n",
    "Y_pred=tf.nn.xw_plus_b(H1,weight4,bias4)\n",
    "\n",
    "\n",
    "# Y=tf.placeholder(tf.float32,shape=[None,1],name=\"Y\")\n",
    "# action=tf.placeholder(tf.int32,shape=[1],name=\"action\")\n",
    "\n",
    "weight01=tf.get_variable(\"w01\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias01=tf.get_variable(\"b01\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight02=tf.get_variable(\"w02\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias02=tf.get_variable(\"b02\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight03=tf.get_variable(\"w03\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias03=tf.get_variable(\"b03\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight04=tf.get_variable(\"w04\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias04=tf.get_variable(\"b04\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist0=[weight01,weight02,weight03,weight04,bias01,bias02,bias03,bias04]\n",
    "#varlist=[weight01,weight04,bias01,bias04]\n",
    "H01=tf.nn.relu(tf.nn.xw_plus_b(X,weight01,bias01))\n",
    "#H02=tf.nn.relu(tf.nn.xw_plus_b(H01,weight02,bias02))\n",
    "#H03=tf.nn.relu(tf.nn.xw_plus_b(H02,weight03,bias03))\n",
    "#Y_pred0=tf.nn.xw_plus_b(H03,weight04,bias04)\n",
    "Y_pred0=tf.nn.xw_plus_b(H01,weight04,bias04)\n",
    "\n",
    "assw1=weight01.assign(weight1)\n",
    "assw2=weight02.assign(weight2)\n",
    "assw3=weight03.assign(weight3)\n",
    "assw4=weight04.assign(weight4)\n",
    "assb1=bias01.assign(bias1)\n",
    "assb2=bias02.assign(bias2)\n",
    "assb3=bias03.assign(bias3)\n",
    "assb4=bias04.assign(bias4)\n",
    "\n",
    "#loss=tf.square(Y-Y_pred)\n",
    "loss=tf.reduce_mean(tf.square(Y-Y_pred))\n",
    "# loss=tf.square(Y-Y_pred[action])\n",
    "#opt=tf.train.AdadeltaOptimizer(1.0).minimize(loss,var_list=varlist)\n",
    "#opt=tf.train.GradientDescentOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "#opt=tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, step: 16, total_reward: -2.150000, epsilon: 0.990000\n",
      "Episode: 1, step: 7, total_reward: -2.060000, epsilon: 0.980100\n",
      "Episode: 2, step: 21, total_reward: -2.200000, epsilon: 0.970299\n",
      "Episode: 3, step: 8, total_reward: -2.070000, epsilon: 0.960596\n",
      "Episode: 4, step: 10, total_reward: -2.090000, epsilon: 0.950990\n",
      "Episode: 5, step: 12, total_reward: -2.110000, epsilon: 0.941480\n",
      "Episode: 6, step: 5, total_reward: -2.040000, epsilon: 0.932065\n",
      "Episode: 7, step: 7, total_reward: -2.060000, epsilon: 0.922745\n",
      "Episode: 8, step: 12, total_reward: -2.110000, epsilon: 0.913517\n",
      "Episode: 9, step: 2, total_reward: -2.010000, epsilon: 0.904382\n",
      "Episode: 10, step: 12, total_reward: 0.890000, epsilon: 0.895338\n",
      "[0, 1, 0, 0, 4, 8, 9, 13, 13, 9, 10, 14, 15]\n",
      "Episode: 11, step: 2, total_reward: -2.010000, epsilon: 0.886385\n",
      "Episode: 12, step: 4, total_reward: -2.030000, epsilon: 0.877521\n",
      "Episode: 13, step: 6, total_reward: -2.050000, epsilon: 0.868746\n",
      "Episode: 14, step: 6, total_reward: -2.050000, epsilon: 0.860058\n",
      "Episode: 15, step: 12, total_reward: -2.110000, epsilon: 0.851458\n",
      "Episode: 16, step: 2, total_reward: -2.010000, epsilon: 0.842943\n",
      "Episode: 17, step: 2, total_reward: -2.010000, epsilon: 0.834514\n",
      "Episode: 18, step: 9, total_reward: -2.080000, epsilon: 0.826169\n",
      "Episode: 19, step: 8, total_reward: -2.070000, epsilon: 0.817907\n",
      "Episode: 20, step: 9, total_reward: -2.080000, epsilon: 0.809728\n",
      "Episode: 21, step: 8, total_reward: -2.070000, epsilon: 0.801631\n",
      "Episode: 22, step: 4, total_reward: -2.030000, epsilon: 0.793614\n",
      "Episode: 23, step: 7, total_reward: -2.060000, epsilon: 0.785678\n",
      "Episode: 24, step: 9, total_reward: -2.080000, epsilon: 0.777821\n",
      "Episode: 25, step: 4, total_reward: -2.030000, epsilon: 0.770043\n",
      "Episode: 26, step: 12, total_reward: -2.110000, epsilon: 0.762343\n",
      "Episode: 27, step: 27, total_reward: -2.260000, epsilon: 0.754719\n",
      "Episode: 28, step: 7, total_reward: -2.060000, epsilon: 0.747172\n",
      "Episode: 29, step: 4, total_reward: -2.030000, epsilon: 0.739700\n",
      "Episode: 30, step: 9, total_reward: -2.080000, epsilon: 0.732303\n",
      "Episode: 31, step: 17, total_reward: -2.160000, epsilon: 0.724980\n",
      "Episode: 32, step: 3, total_reward: -2.020000, epsilon: 0.717731\n",
      "Episode: 33, step: 13, total_reward: -2.120000, epsilon: 0.710553\n",
      "Episode: 34, step: 3, total_reward: -2.020000, epsilon: 0.703448\n",
      "Episode: 35, step: 9, total_reward: -2.080000, epsilon: 0.696413\n",
      "Episode: 36, step: 2, total_reward: -2.010000, epsilon: 0.689449\n",
      "Episode: 37, step: 3, total_reward: -2.020000, epsilon: 0.682555\n",
      "Episode: 38, step: 11, total_reward: -2.100000, epsilon: 0.675729\n",
      "Episode: 39, step: 5, total_reward: -2.040000, epsilon: 0.668972\n",
      "Episode: 40, step: 6, total_reward: -2.050000, epsilon: 0.662282\n",
      "Episode: 41, step: 22, total_reward: -2.210000, epsilon: 0.655659\n",
      "Episode: 42, step: 5, total_reward: -2.040000, epsilon: 0.649103\n",
      "Episode: 43, step: 9, total_reward: -2.080000, epsilon: 0.642612\n",
      "Episode: 44, step: 6, total_reward: -2.050000, epsilon: 0.636185\n",
      "Episode: 45, step: 16, total_reward: -2.150000, epsilon: 0.629824\n",
      "Episode: 46, step: 11, total_reward: -2.100000, epsilon: 0.623525\n",
      "Episode: 47, step: 18, total_reward: -2.170000, epsilon: 0.617290\n",
      "Episode: 48, step: 14, total_reward: -2.130000, epsilon: 0.611117\n",
      "Episode: 49, step: 3, total_reward: -2.020000, epsilon: 0.605006\n",
      "Episode: 50, step: 15, total_reward: -2.140000, epsilon: 0.598956\n",
      "Episode: 51, step: 4, total_reward: -2.030000, epsilon: 0.592966\n",
      "Episode: 52, step: 9, total_reward: -2.080000, epsilon: 0.587037\n",
      "Episode: 53, step: 9, total_reward: -2.080000, epsilon: 0.581166\n",
      "Episode: 54, step: 27, total_reward: -2.260000, epsilon: 0.575355\n",
      "Episode: 55, step: 28, total_reward: -2.270000, epsilon: 0.569601\n",
      "Episode: 56, step: 5, total_reward: -2.040000, epsilon: 0.563905\n",
      "Episode: 57, step: 7, total_reward: -2.060000, epsilon: 0.558266\n",
      "Episode: 58, step: 17, total_reward: -2.160000, epsilon: 0.552683\n",
      "Episode: 59, step: 5, total_reward: -2.040000, epsilon: 0.547157\n",
      "Episode: 60, step: 4, total_reward: -2.030000, epsilon: 0.541685\n",
      "Episode: 61, step: 9, total_reward: -2.080000, epsilon: 0.536268\n",
      "Episode: 62, step: 11, total_reward: -2.100000, epsilon: 0.530906\n",
      "Episode: 63, step: 7, total_reward: -2.060000, epsilon: 0.525596\n",
      "Episode: 64, step: 8, total_reward: -2.070000, epsilon: 0.520341\n",
      "Episode: 65, step: 5, total_reward: -2.040000, epsilon: 0.515137\n",
      "Episode: 66, step: 4, total_reward: -2.030000, epsilon: 0.509986\n",
      "Episode: 67, step: 41, total_reward: -2.400000, epsilon: 0.504886\n",
      "Episode: 68, step: 9, total_reward: -2.080000, epsilon: 0.499837\n",
      "Episode: 69, step: 4, total_reward: -2.030000, epsilon: 0.494839\n",
      "Episode: 70, step: 4, total_reward: -2.030000, epsilon: 0.489890\n",
      "Episode: 71, step: 17, total_reward: -2.160000, epsilon: 0.484991\n",
      "Episode: 72, step: 11, total_reward: -2.100000, epsilon: 0.480141\n",
      "Episode: 73, step: 4, total_reward: -2.030000, epsilon: 0.475340\n",
      "Episode: 74, step: 3, total_reward: -2.020000, epsilon: 0.470587\n",
      "Episode: 75, step: 10, total_reward: -2.090000, epsilon: 0.465881\n",
      "Episode: 76, step: 2, total_reward: -2.010000, epsilon: 0.461222\n",
      "Episode: 77, step: 13, total_reward: -2.120000, epsilon: 0.456610\n",
      "Episode: 78, step: 2, total_reward: -2.010000, epsilon: 0.452044\n",
      "Episode: 79, step: 8, total_reward: -2.070000, epsilon: 0.447523\n",
      "Episode: 80, step: 28, total_reward: -2.270000, epsilon: 0.443048\n",
      "Episode: 81, step: 7, total_reward: -2.060000, epsilon: 0.438618\n",
      "Episode: 82, step: 16, total_reward: -2.150000, epsilon: 0.434231\n",
      "Episode: 83, step: 3, total_reward: -2.020000, epsilon: 0.429889\n",
      "Episode: 84, step: 3, total_reward: -2.020000, epsilon: 0.425590\n",
      "Episode: 85, step: 6, total_reward: -2.050000, epsilon: 0.421334\n",
      "Episode: 86, step: 16, total_reward: -2.150000, epsilon: 0.417121\n",
      "Episode: 87, step: 53, total_reward: -2.520000, epsilon: 0.412950\n",
      "Episode: 88, step: 35, total_reward: -2.340000, epsilon: 0.408820\n",
      "Episode: 89, step: 19, total_reward: -2.180000, epsilon: 0.404732\n",
      "Episode: 90, step: 64, total_reward: -2.630000, epsilon: 0.400685\n",
      "Episode: 91, step: 17, total_reward: -2.160000, epsilon: 0.396678\n",
      "Episode: 92, step: 13, total_reward: -2.120000, epsilon: 0.392711\n",
      "Episode: 93, step: 31, total_reward: 0.700000, epsilon: 0.388784\n",
      "[0, 0, 4, 8, 4, 8, 4, 8, 9, 13, 9, 13, 14, 14, 10, 14, 13, 13, 13, 14, 13, 13, 13, 14, 13, 14, 14, 13, 13, 13, 14, 15]\n",
      "Episode: 94, step: 13, total_reward: -2.120000, epsilon: 0.384896\n",
      "Episode: 95, step: 44, total_reward: -2.430000, epsilon: 0.381047\n",
      "Episode: 96, step: 6, total_reward: -2.050000, epsilon: 0.377237\n",
      "Episode: 97, step: 4, total_reward: -2.030000, epsilon: 0.373464\n",
      "Episode: 98, step: 6, total_reward: -2.050000, epsilon: 0.369730\n",
      "Episode: 99, step: 7, total_reward: -2.060000, epsilon: 0.366032\n",
      "Episode: 100, step: 15, total_reward: -2.140000, epsilon: 0.362372\n",
      "Episode: 101, step: 15, total_reward: -2.140000, epsilon: 0.358748\n",
      "Episode: 102, step: 22, total_reward: -2.210000, epsilon: 0.355161\n",
      "Episode: 103, step: 12, total_reward: -2.110000, epsilon: 0.351609\n",
      "Episode: 104, step: 12, total_reward: -2.110000, epsilon: 0.348093\n",
      "Episode: 105, step: 31, total_reward: 0.700000, epsilon: 0.344612\n",
      "[0, 4, 4, 8, 4, 8, 9, 10, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 9, 10, 9, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15]\n",
      "Episode: 106, step: 12, total_reward: -2.110000, epsilon: 0.341166\n",
      "Episode: 107, step: 19, total_reward: -2.180000, epsilon: 0.337754\n",
      "Episode: 108, step: 24, total_reward: -2.230000, epsilon: 0.334377\n",
      "Episode: 109, step: 2, total_reward: -2.010000, epsilon: 0.331033\n",
      "Episode: 110, step: 17, total_reward: -2.160000, epsilon: 0.327723\n",
      "Episode: 111, step: 40, total_reward: -2.390000, epsilon: 0.324446\n",
      "Episode: 112, step: 37, total_reward: -2.360000, epsilon: 0.321201\n",
      "Episode: 113, step: 3, total_reward: -2.020000, epsilon: 0.317989\n",
      "Episode: 114, step: 16, total_reward: -2.150000, epsilon: 0.314809\n",
      "Episode: 115, step: 26, total_reward: -2.250000, epsilon: 0.311661\n",
      "Episode: 116, step: 9, total_reward: -2.080000, epsilon: 0.308544\n",
      "Episode: 117, step: 13, total_reward: -2.120000, epsilon: 0.305459\n",
      "Episode: 118, step: 4, total_reward: -2.030000, epsilon: 0.302404\n",
      "Episode: 119, step: 3, total_reward: -2.020000, epsilon: 0.299380\n",
      "Episode: 120, step: 25, total_reward: -2.240000, epsilon: 0.296387\n",
      "Episode: 121, step: 29, total_reward: -2.280000, epsilon: 0.293423\n",
      "Episode: 122, step: 47, total_reward: -2.460000, epsilon: 0.290488\n",
      "Episode: 123, step: 27, total_reward: -2.260000, epsilon: 0.287584\n",
      "Episode: 124, step: 100, total_reward: -2.990000, epsilon: 0.284708\n",
      "Episode: 125, step: 16, total_reward: -2.150000, epsilon: 0.281861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 126, step: 15, total_reward: -2.140000, epsilon: 0.279042\n",
      "Episode: 127, step: 21, total_reward: -2.200000, epsilon: 0.276252\n",
      "Episode: 128, step: 32, total_reward: -2.310000, epsilon: 0.273489\n",
      "Episode: 129, step: 11, total_reward: -2.100000, epsilon: 0.270754\n",
      "Episode: 130, step: 31, total_reward: -2.300000, epsilon: 0.268047\n",
      "Episode: 131, step: 8, total_reward: -2.070000, epsilon: 0.265366\n",
      "Episode: 132, step: 54, total_reward: -2.530000, epsilon: 0.262713\n",
      "Episode: 133, step: 2, total_reward: -2.010000, epsilon: 0.260085\n",
      "Episode: 134, step: 10, total_reward: -2.090000, epsilon: 0.257485\n",
      "Episode: 135, step: 6, total_reward: -2.050000, epsilon: 0.254910\n",
      "Episode: 136, step: 17, total_reward: -2.160000, epsilon: 0.252361\n",
      "Episode: 137, step: 25, total_reward: -2.240000, epsilon: 0.249837\n",
      "Episode: 138, step: 30, total_reward: -2.290000, epsilon: 0.247339\n",
      "Episode: 139, step: 24, total_reward: -2.230000, epsilon: 0.244865\n",
      "Episode: 140, step: 13, total_reward: -2.120000, epsilon: 0.242417\n",
      "Episode: 141, step: 12, total_reward: -2.110000, epsilon: 0.239992\n",
      "Episode: 142, step: 2, total_reward: -2.010000, epsilon: 0.237593\n",
      "Episode: 143, step: 2, total_reward: -2.010000, epsilon: 0.235217\n",
      "Episode: 144, step: 4, total_reward: -2.030000, epsilon: 0.232864\n",
      "Episode: 145, step: 3, total_reward: -2.020000, epsilon: 0.230536\n",
      "Episode: 146, step: 4, total_reward: -2.030000, epsilon: 0.228230\n",
      "Episode: 147, step: 32, total_reward: -2.310000, epsilon: 0.225948\n",
      "Episode: 148, step: 3, total_reward: -2.020000, epsilon: 0.223689\n",
      "Episode: 149, step: 15, total_reward: -2.140000, epsilon: 0.221452\n",
      "Episode: 150, step: 7, total_reward: -2.060000, epsilon: 0.219237\n",
      "Episode: 151, step: 12, total_reward: -2.110000, epsilon: 0.217045\n",
      "Episode: 152, step: 11, total_reward: -2.100000, epsilon: 0.214874\n",
      "Episode: 153, step: 51, total_reward: -2.500000, epsilon: 0.212726\n",
      "Episode: 154, step: 7, total_reward: -2.060000, epsilon: 0.210598\n",
      "Episode: 155, step: 4, total_reward: -2.030000, epsilon: 0.208492\n",
      "Episode: 156, step: 23, total_reward: -2.220000, epsilon: 0.206408\n",
      "Episode: 157, step: 39, total_reward: -2.380000, epsilon: 0.204343\n",
      "Episode: 158, step: 18, total_reward: -2.170000, epsilon: 0.202300\n",
      "Episode: 159, step: 67, total_reward: -2.660000, epsilon: 0.200277\n",
      "Episode: 160, step: 60, total_reward: -2.590000, epsilon: 0.198274\n",
      "Episode: 161, step: 100, total_reward: -1.000000, epsilon: 0.196292\n",
      "Episode: 162, step: 36, total_reward: -2.350000, epsilon: 0.194329\n",
      "Episode: 163, step: 13, total_reward: -2.120000, epsilon: 0.192385\n",
      "Episode: 164, step: 39, total_reward: -2.380000, epsilon: 0.190461\n",
      "Episode: 165, step: 66, total_reward: -2.650000, epsilon: 0.188557\n",
      "Episode: 166, step: 9, total_reward: -2.080000, epsilon: 0.186671\n",
      "Episode: 167, step: 31, total_reward: -2.300000, epsilon: 0.184805\n",
      "Episode: 168, step: 20, total_reward: -2.190000, epsilon: 0.182957\n",
      "Episode: 169, step: 14, total_reward: -2.130000, epsilon: 0.181127\n",
      "Episode: 170, step: 100, total_reward: -1.000000, epsilon: 0.179316\n",
      "Episode: 171, step: 20, total_reward: -2.190000, epsilon: 0.177523\n",
      "Episode: 172, step: 21, total_reward: -2.200000, epsilon: 0.175747\n",
      "Episode: 173, step: 7, total_reward: -2.060000, epsilon: 0.173990\n",
      "Episode: 174, step: 37, total_reward: -2.360000, epsilon: 0.172250\n",
      "Episode: 175, step: 11, total_reward: -2.100000, epsilon: 0.170527\n",
      "Episode: 176, step: 7, total_reward: -2.060000, epsilon: 0.168822\n",
      "Episode: 177, step: 4, total_reward: -2.030000, epsilon: 0.167134\n",
      "Episode: 178, step: 9, total_reward: -2.080000, epsilon: 0.165463\n",
      "Episode: 179, step: 41, total_reward: -2.400000, epsilon: 0.163808\n",
      "Episode: 180, step: 26, total_reward: -2.250000, epsilon: 0.162170\n",
      "Episode: 181, step: 29, total_reward: -2.280000, epsilon: 0.160548\n",
      "Episode: 182, step: 33, total_reward: -2.320000, epsilon: 0.158943\n",
      "Episode: 183, step: 100, total_reward: -1.000000, epsilon: 0.157353\n",
      "Episode: 184, step: 62, total_reward: -2.610000, epsilon: 0.155780\n",
      "Episode: 185, step: 17, total_reward: -2.160000, epsilon: 0.154222\n",
      "Episode: 186, step: 12, total_reward: -2.110000, epsilon: 0.152680\n",
      "Episode: 187, step: 27, total_reward: -2.260000, epsilon: 0.151153\n",
      "Episode: 188, step: 100, total_reward: -1.000000, epsilon: 0.149641\n",
      "Episode: 189, step: 39, total_reward: -2.380000, epsilon: 0.148145\n",
      "Episode: 190, step: 12, total_reward: -2.110000, epsilon: 0.146664\n",
      "Episode: 191, step: 6, total_reward: -2.050000, epsilon: 0.145197\n",
      "Episode: 192, step: 13, total_reward: -2.120000, epsilon: 0.143745\n",
      "Episode: 193, step: 5, total_reward: -2.040000, epsilon: 0.142307\n",
      "Episode: 194, step: 7, total_reward: -2.060000, epsilon: 0.140884\n",
      "Episode: 195, step: 7, total_reward: -2.060000, epsilon: 0.139476\n",
      "Episode: 196, step: 7, total_reward: -2.060000, epsilon: 0.138081\n",
      "Episode: 197, step: 8, total_reward: -2.070000, epsilon: 0.136700\n",
      "Episode: 198, step: 14, total_reward: -2.130000, epsilon: 0.135333\n",
      "Episode: 199, step: 49, total_reward: -2.480000, epsilon: 0.133980\n",
      "Episode: 200, step: 61, total_reward: -2.600000, epsilon: 0.132640\n",
      "Episode: 201, step: 46, total_reward: -2.450000, epsilon: 0.131313\n",
      "Episode: 202, step: 81, total_reward: -2.800000, epsilon: 0.130000\n",
      "Episode: 203, step: 3, total_reward: -2.020000, epsilon: 0.128700\n",
      "Episode: 204, step: 75, total_reward: -2.740000, epsilon: 0.127413\n",
      "Episode: 205, step: 7, total_reward: -2.060000, epsilon: 0.126139\n",
      "Episode: 206, step: 62, total_reward: -2.610000, epsilon: 0.124878\n",
      "Episode: 207, step: 35, total_reward: -2.340000, epsilon: 0.123629\n",
      "Episode: 208, step: 49, total_reward: -2.480000, epsilon: 0.122393\n",
      "Episode: 209, step: 34, total_reward: -2.330000, epsilon: 0.121169\n",
      "Episode: 210, step: 70, total_reward: -2.690000, epsilon: 0.119957\n",
      "Episode: 211, step: 35, total_reward: -2.340000, epsilon: 0.118758\n",
      "Episode: 212, step: 3, total_reward: -2.020000, epsilon: 0.117570\n",
      "Episode: 213, step: 4, total_reward: -2.030000, epsilon: 0.116394\n",
      "Episode: 214, step: 80, total_reward: -2.790000, epsilon: 0.115230\n",
      "Episode: 215, step: 16, total_reward: -2.150000, epsilon: 0.114078\n",
      "Episode: 216, step: 15, total_reward: -2.140000, epsilon: 0.112937\n",
      "Episode: 217, step: 21, total_reward: -2.200000, epsilon: 0.111808\n",
      "Episode: 218, step: 10, total_reward: -2.090000, epsilon: 0.110690\n",
      "Episode: 219, step: 13, total_reward: -2.120000, epsilon: 0.109583\n",
      "Episode: 220, step: 100, total_reward: -2.990000, epsilon: 0.108487\n",
      "Episode: 221, step: 54, total_reward: -2.530000, epsilon: 0.107402\n",
      "Episode: 222, step: 23, total_reward: -2.220000, epsilon: 0.106328\n",
      "Episode: 223, step: 92, total_reward: -2.910000, epsilon: 0.105265\n",
      "Episode: 224, step: 12, total_reward: -2.110000, epsilon: 0.104212\n",
      "Episode: 225, step: 14, total_reward: -2.130000, epsilon: 0.103170\n",
      "Episode: 226, step: 61, total_reward: -2.600000, epsilon: 0.102138\n",
      "Episode: 227, step: 100, total_reward: -1.000000, epsilon: 0.101117\n",
      "Episode: 228, step: 27, total_reward: -2.260000, epsilon: 0.100106\n",
      "Episode: 229, step: 66, total_reward: -2.650000, epsilon: 0.099105\n",
      "Episode: 230, step: 18, total_reward: -2.170000, epsilon: 0.098114\n",
      "Episode: 231, step: 16, total_reward: -2.150000, epsilon: 0.097133\n",
      "Episode: 232, step: 12, total_reward: -2.110000, epsilon: 0.096161\n",
      "Episode: 233, step: 100, total_reward: -1.000000, epsilon: 0.095200\n",
      "Episode: 234, step: 37, total_reward: -2.360000, epsilon: 0.094248\n",
      "Episode: 235, step: 24, total_reward: -2.230000, epsilon: 0.093305\n",
      "Episode: 236, step: 32, total_reward: 0.690000, epsilon: 0.092372\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 6, 10, 9, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15]\n",
      "Episode: 237, step: 100, total_reward: -1.000000, epsilon: 0.091448\n",
      "Episode: 238, step: 31, total_reward: -2.300000, epsilon: 0.090534\n",
      "Episode: 239, step: 43, total_reward: -2.420000, epsilon: 0.089629\n",
      "Episode: 240, step: 100, total_reward: -1.000000, epsilon: 0.088732\n",
      "Episode: 241, step: 49, total_reward: -2.480000, epsilon: 0.087845\n",
      "Episode: 242, step: 19, total_reward: -2.180000, epsilon: 0.086967\n",
      "Episode: 243, step: 11, total_reward: -2.100000, epsilon: 0.086097\n",
      "Episode: 244, step: 71, total_reward: -2.700000, epsilon: 0.085236\n",
      "Episode: 245, step: 28, total_reward: -2.270000, epsilon: 0.084384\n",
      "Episode: 246, step: 100, total_reward: -1.000000, epsilon: 0.083540\n",
      "Episode: 247, step: 36, total_reward: -2.350000, epsilon: 0.082704\n",
      "Episode: 248, step: 57, total_reward: -2.560000, epsilon: 0.081877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 249, step: 100, total_reward: -1.000000, epsilon: 0.081059\n",
      "Episode: 250, step: 16, total_reward: -2.150000, epsilon: 0.080248\n",
      "Episode: 251, step: 100, total_reward: -1.000000, epsilon: 0.079445\n",
      "Episode: 252, step: 100, total_reward: -1.000000, epsilon: 0.078651\n",
      "Episode: 253, step: 100, total_reward: -1.000000, epsilon: 0.077864\n",
      "Episode: 254, step: 72, total_reward: -2.710000, epsilon: 0.077086\n",
      "Episode: 255, step: 96, total_reward: -2.950000, epsilon: 0.076315\n",
      "Episode: 256, step: 100, total_reward: -1.000000, epsilon: 0.075552\n",
      "Episode: 257, step: 62, total_reward: -2.610000, epsilon: 0.074796\n",
      "Episode: 258, step: 100, total_reward: -1.000000, epsilon: 0.074048\n",
      "Episode: 259, step: 100, total_reward: -1.000000, epsilon: 0.073308\n",
      "Episode: 260, step: 15, total_reward: -2.140000, epsilon: 0.072575\n",
      "Episode: 261, step: 31, total_reward: -2.300000, epsilon: 0.071849\n",
      "Episode: 262, step: 77, total_reward: -2.760000, epsilon: 0.071131\n",
      "Episode: 263, step: 100, total_reward: -1.000000, epsilon: 0.070419\n",
      "Episode: 264, step: 81, total_reward: -2.800000, epsilon: 0.069715\n",
      "Episode: 265, step: 100, total_reward: -1.000000, epsilon: 0.069018\n",
      "Episode: 266, step: 72, total_reward: -2.710000, epsilon: 0.068328\n",
      "Episode: 267, step: 6, total_reward: -2.050000, epsilon: 0.067644\n",
      "Episode: 268, step: 94, total_reward: -2.930000, epsilon: 0.066968\n",
      "Episode: 269, step: 14, total_reward: -2.130000, epsilon: 0.066298\n",
      "Episode: 270, step: 2, total_reward: -2.010000, epsilon: 0.065635\n",
      "Episode: 271, step: 100, total_reward: -1.000000, epsilon: 0.064979\n",
      "Episode: 272, step: 53, total_reward: -2.520000, epsilon: 0.064329\n",
      "Episode: 273, step: 20, total_reward: -2.190000, epsilon: 0.063686\n",
      "Episode: 274, step: 49, total_reward: -2.480000, epsilon: 0.063049\n",
      "Episode: 275, step: 61, total_reward: -2.600000, epsilon: 0.062419\n",
      "Episode: 276, step: 46, total_reward: -2.450000, epsilon: 0.061794\n",
      "Episode: 277, step: 100, total_reward: -1.000000, epsilon: 0.061176\n",
      "Episode: 278, step: 21, total_reward: -2.200000, epsilon: 0.060565\n",
      "Episode: 279, step: 26, total_reward: -2.250000, epsilon: 0.059959\n",
      "Episode: 280, step: 52, total_reward: -2.510000, epsilon: 0.059359\n",
      "Episode: 281, step: 74, total_reward: -2.730000, epsilon: 0.058766\n",
      "Episode: 282, step: 80, total_reward: -2.790000, epsilon: 0.058178\n",
      "Episode: 283, step: 31, total_reward: -2.300000, epsilon: 0.057596\n",
      "Episode: 284, step: 57, total_reward: -2.560000, epsilon: 0.057020\n",
      "Episode: 285, step: 18, total_reward: -2.170000, epsilon: 0.056450\n",
      "Episode: 286, step: 15, total_reward: -2.140000, epsilon: 0.055886\n",
      "Episode: 287, step: 95, total_reward: -2.940000, epsilon: 0.055327\n",
      "Episode: 288, step: 10, total_reward: -2.090000, epsilon: 0.054774\n",
      "Episode: 289, step: 46, total_reward: -2.450000, epsilon: 0.054226\n",
      "Episode: 290, step: 31, total_reward: -2.300000, epsilon: 0.053684\n",
      "Episode: 291, step: 48, total_reward: -2.470000, epsilon: 0.053147\n",
      "Episode: 292, step: 6, total_reward: -2.050000, epsilon: 0.052615\n",
      "Episode: 293, step: 46, total_reward: -2.450000, epsilon: 0.052089\n",
      "Episode: 294, step: 2, total_reward: -2.010000, epsilon: 0.051568\n",
      "Episode: 295, step: 79, total_reward: -2.780000, epsilon: 0.051053\n",
      "Episode: 296, step: 4, total_reward: -2.030000, epsilon: 0.050542\n",
      "Episode: 297, step: 100, total_reward: -1.000000, epsilon: 0.050037\n",
      "Episode: 298, step: 7, total_reward: -2.060000, epsilon: 0.049536\n",
      "Episode: 299, step: 33, total_reward: -2.320000, epsilon: 0.049041\n",
      "min step:  12 min step episode 10\n",
      "shortest path:  [0, 1, 0, 0, 4, 8, 9, 13, 13, 9, 10, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=300\n",
    "max_step=100\n",
    "gamma=0.995\n",
    "epsilon0 = 1.0\n",
    "sess.run(init)\n",
    "sess.run([assw1,assw2,assw3,assw4,assb1,assb2,assb3,assb4])\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "max_replay=500\n",
    "replay=deque(maxlen=max_replay)\n",
    "#replay=[]\n",
    "replay0=deque(maxlen=max_replay)\n",
    "replay1=deque(maxlen=max_replay)\n",
    "replay2=deque(maxlen=max_replay)\n",
    "i_step=0\n",
    "batch_size=10\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "    \n",
    "    state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "    state_m[0][state]=1.0\n",
    "    Q_pred=sess.run(Y_pred0,feed_dict={X:state_m})\n",
    "    if np.random.rand()>epsilon :\n",
    "        action=np.argmax(Q_pred[0])\n",
    "    else:\n",
    "        action=np.random.choice(env.action_space.n) \n",
    "    for step in range(1,max_step+1):\n",
    "        \n",
    "        i_step+=1\n",
    "   \n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "        \n",
    "        if reward==0 : \n",
    "            reward=-0.01\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-2\n",
    "        \n",
    "        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m_next[0][state_next]=1.0\n",
    "        Q_pred_next=sess.run(Y_pred0, feed_dict={X:state_m_next}) \n",
    "        if np.random.rand()>epsilon :\n",
    "            action_next=np.argmax(Q_pred_next[0])\n",
    "        else:\n",
    "            action_next=np.random.choice(env.action_space.n)\n",
    "        \n",
    "        Y_true=copy.deepcopy(Q_pred[0])\n",
    "        if reward==1.0 and game_over==1:\n",
    "            Y_true[action]= reward\n",
    "        else :\n",
    "            Y_true[action]=reward+gamma* (Q_pred_next[0][action_next])\n",
    "        Y_true=np.reshape(Y_true,[1,env.action_space.n])\n",
    "\n",
    "        replay.append([state_m[0],Y_true])\n",
    "\n",
    "#        if state in replay0:\n",
    "#            index=replay0.index(state)\n",
    "#            replay[index]=[state_m[0],Y_true]\n",
    "#        else:\n",
    "#            replay0.append(state)\n",
    "#            replay.append([state_m[0],Y_true]) \n",
    "        \n",
    "#            game_over=0\n",
    "        # Q-Learning\n",
    "        if i_step%20==0 and i_step!=0 and len(replay)>batch_size:\n",
    "#            print(replay0)\n",
    "            mini_batch = random.sample(replay,batch_size)\n",
    "            states=np.zeros([batch_size,inputdim])\n",
    "            Qs=np.zeros([batch_size,outputdim])\n",
    "            for m in range(batch_size):\n",
    "                states[m]=mini_batch[m][0]\n",
    "                Qs[m]=mini_batch[m][1]\n",
    "            cost0= sess.run([loss],feed_dict={X:states,Y:Qs})\n",
    "            for i in range(0,2):\n",
    "                sess.run([opt],feed_dict={X:states,Y:Qs})\n",
    "                cost= sess.run([loss],feed_dict={X:states,Y:Qs})\n",
    "#            Q_pred=sess.run(Y_pred,feed_dict={X:states})\n",
    "            #print(i_step,cost0,cost,Qs[0],Q_pred[0])\n",
    "#            print(i_step,cost0,cost)\n",
    "\n",
    "        if i_step%100==0  :\n",
    "#            print(i_step,\"update\")\n",
    "            sess.run([assw1,assw2,assw3,assw4,assb1,assb2,assb3,assb4])\n",
    "            \n",
    "        state=state_next\n",
    "        state_m=state_m_next\n",
    "        action=action_next\n",
    "        \n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "            \n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n",
    "#print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[-0.3193047   0.0772663   0.12334345  0.1268539 ]\n",
      " [-0.32311273  0.09331876  0.17124411  0.11340132]\n",
      " [-0.3256859   0.06620821  0.1321699   0.12647435]\n",
      " [-0.32298675  0.05508754  0.17610991  0.10357158]\n",
      " [-0.23784913  0.08147555 -0.21349175  0.16446747]\n",
      " [-0.14482181 -0.00185806  0.23423135 -0.10606202]\n",
      " [-0.31880724  0.08778637  0.1956693   0.17126487]\n",
      " [-0.28401023  0.0964123  -0.13534428 -0.0260882 ]\n",
      " [-0.38892242  0.04145909  0.15882131  0.24629675]\n",
      " [-0.09056944  0.29875803  0.16717696 -0.00319835]\n",
      " [-0.42414215  0.11395315  0.24345335  0.20396113]\n",
      " [-0.20755486  0.10998899  0.24761568  0.14405157]\n",
      " [-0.37244168  0.24509059  0.34299713  0.25176606]\n",
      " [-0.39547226  0.1354162   0.7089015   0.30775604]\n",
      " [-0.34181517  0.12630263  0.7805561   0.2928009 ]\n",
      " [ 0.0197199  -0.00654274  0.02214172 -0.00803148]]\n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([16,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,15):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "print(Q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
