{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "#env =FrozenLakeEnv(is_slippery=False,map_name=\"8x8\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,para,bias=True):\n",
    "        super(Net,self).__init__()\n",
    "        self.input_dim=para['input_dim']\n",
    "        self.output_dim=para['output_dim']\n",
    "        self.hidden_dim=para['hidden_dim']\n",
    "        \n",
    "        self.w1=Parameter(torch.FloatTensor(self.input_dim,self.hidden_dim))\n",
    "        self.b1=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w1)\n",
    "#        nn.init.kaiming_uniform_(self.w1)\n",
    "        nn.init.uniform_(self.b1,-0.001,0.001)\n",
    "\n",
    "        self.w2=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b2=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w2)\n",
    "#        nn.init.kaiming_uniform_(self.w2)\n",
    "        nn.init.uniform_(self.b2,-0.001,0.001)\n",
    "        \n",
    "        self.w3=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b3=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w3)\n",
    "#        nn.init.kaiming_uniform_(self.w3)\n",
    "        nn.init.uniform_(self.b3,-0.001,0.001)\n",
    "        \n",
    "        self.w4=Parameter(torch.FloatTensor(self.hidden_dim,self.output_dim))\n",
    "        self.b4=Parameter(torch.FloatTensor(self.output_dim))\n",
    "        nn.init.kaiming_normal_(self.w4)\n",
    "#        nn.init.kaiming_uniform_(self.w4)\n",
    "        nn.init.uniform_(self.b4,-0.001,0.001)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        H=torch.relu( torch.einsum('li,ij->lj',X,self.w1)+self.b1)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w2)+self.b2)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w3)+self.b3)\n",
    "        Y=torch.einsum('li,ij->lj',H,self.w4)+self.b4\n",
    "        Y=torch.softmax(Y,dim=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=env.observation_space.n\n",
    "output_dim=env.action_space.n\n",
    "hidden_dim=100\n",
    "lr=0.0001\n",
    "lr2=1.0\n",
    "\n",
    "para={'input_dim': input_dim, 'output_dim': output_dim, 'hidden_dim': hidden_dim}\n",
    "model=Net(para)\n",
    "#criterion=nn.MSELoss()\n",
    "#criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0 33 tensor(-33.7779)\n",
      "Episode: 0, step: 33, total_reward: -4.300000, epsilon: 0.990000\n",
      "99\n",
      "1 99 tensor(-50.5684)\n",
      "Episode: 1, step: 99, total_reward: -10.000000, epsilon: 0.980100\n",
      "99\n",
      "2 99 tensor(-30.7379)\n",
      "Episode: 2, step: 99, total_reward: -10.000000, epsilon: 0.970299\n",
      "99\n",
      "3 99 tensor(-36.4557)\n",
      "Episode: 3, step: 99, total_reward: -10.000000, epsilon: 0.960596\n",
      "99\n",
      "4 99 tensor(-8.3767)\n",
      "Episode: 4, step: 99, total_reward: -10.000000, epsilon: 0.950990\n",
      "99\n",
      "5 99 tensor(-67.1364)\n",
      "Episode: 5, step: 99, total_reward: -10.000000, epsilon: 0.941480\n",
      "99\n",
      "6 99 tensor(-86.0227)\n",
      "Episode: 6, step: 99, total_reward: -10.000000, epsilon: 0.932065\n",
      "99\n",
      "7 99 tensor(-31.8685)\n",
      "Episode: 7, step: 99, total_reward: -10.000000, epsilon: 0.922745\n",
      "99\n",
      "8 99 tensor(-7.6570)\n",
      "Episode: 8, step: 99, total_reward: -10.000000, epsilon: 0.913517\n",
      "99\n",
      "9 99 tensor(-82.5905)\n",
      "Episode: 9, step: 99, total_reward: -10.000000, epsilon: 0.904382\n",
      "99\n",
      "10 99 tensor(-20.2719)\n",
      "Episode: 10, step: 99, total_reward: -10.000000, epsilon: 0.895338\n",
      "99\n",
      "11 99 tensor(-28.7450)\n",
      "Episode: 11, step: 99, total_reward: -10.000000, epsilon: 0.886385\n",
      "99\n",
      "12 99 tensor(-7.1963)\n",
      "Episode: 12, step: 99, total_reward: -10.000000, epsilon: 0.877521\n",
      "99\n",
      "13 99 tensor(-31.6547)\n",
      "Episode: 13, step: 99, total_reward: -10.000000, epsilon: 0.868746\n",
      "69\n",
      "14 69 tensor(-51.9472)\n",
      "Episode: 14, step: 69, total_reward: -5.900000, epsilon: 0.860058\n",
      "[0, 0, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 13, 13, 13, 13, 14, 14]\n",
      "99\n",
      "15 99 tensor(-31.1943)\n",
      "Episode: 15, step: 99, total_reward: -10.000000, epsilon: 0.851458\n",
      "91\n",
      "16 91 tensor(-75.7194)\n",
      "Episode: 16, step: 91, total_reward: -10.100000, epsilon: 0.842943\n",
      "99\n",
      "17 99 tensor(-35.2762)\n",
      "Episode: 17, step: 99, total_reward: -10.000000, epsilon: 0.834514\n",
      "99\n",
      "18 99 tensor(-30.1247)\n",
      "Episode: 18, step: 99, total_reward: -10.000000, epsilon: 0.826169\n",
      "99\n",
      "19 99 tensor(-41.9849)\n",
      "Episode: 19, step: 99, total_reward: -10.000000, epsilon: 0.817907\n",
      "99\n",
      "20 99 tensor(-109.0203)\n",
      "Episode: 20, step: 99, total_reward: -10.000000, epsilon: 0.809728\n",
      "99\n",
      "21 99 tensor(-36.2836)\n",
      "Episode: 21, step: 99, total_reward: -10.000000, epsilon: 0.801631\n",
      "99\n",
      "22 99 tensor(-71.9368)\n",
      "Episode: 22, step: 99, total_reward: -10.000000, epsilon: 0.793614\n",
      "99\n",
      "23 99 tensor(-10.5103)\n",
      "Episode: 23, step: 99, total_reward: -10.000000, epsilon: 0.785678\n",
      "99\n",
      "24 99 tensor(-13.2807)\n",
      "Episode: 24, step: 99, total_reward: -10.000000, epsilon: 0.777821\n",
      "99\n",
      "25 99 tensor(-60.9922)\n",
      "Episode: 25, step: 99, total_reward: -10.000000, epsilon: 0.770043\n",
      "99\n",
      "26 99 tensor(-18.6365)\n",
      "Episode: 26, step: 99, total_reward: -10.000000, epsilon: 0.762343\n",
      "99\n",
      "27 99 tensor(-25.9560)\n",
      "Episode: 27, step: 99, total_reward: -10.000000, epsilon: 0.754719\n",
      "99\n",
      "28 99 tensor(-30.6475)\n",
      "Episode: 28, step: 99, total_reward: -10.000000, epsilon: 0.747172\n",
      "99\n",
      "29 99 tensor(-77.8392)\n",
      "Episode: 29, step: 99, total_reward: -10.000000, epsilon: 0.739700\n",
      "8\n",
      "30 8 tensor(-11.2362)\n",
      "Episode: 30, step: 8, total_reward: -1.800000, epsilon: 0.732303\n",
      "99\n",
      "31 99 tensor(-31.4494)\n",
      "Episode: 31, step: 99, total_reward: -10.000000, epsilon: 0.724980\n",
      "99\n",
      "32 99 tensor(-32.6170)\n",
      "Episode: 32, step: 99, total_reward: -10.000000, epsilon: 0.717731\n",
      "9\n",
      "33 9 tensor(-12.3963)\n",
      "Episode: 33, step: 9, total_reward: -1.900000, epsilon: 0.710553\n",
      "99\n",
      "34 99 tensor(-11.1178)\n",
      "Episode: 34, step: 99, total_reward: -10.000000, epsilon: 0.703448\n",
      "83\n",
      "35 83 tensor(-79.0246)\n",
      "Episode: 35, step: 83, total_reward: -9.300000, epsilon: 0.696413\n",
      "99\n",
      "36 99 tensor(-17.8463)\n",
      "Episode: 36, step: 99, total_reward: -10.000000, epsilon: 0.689449\n",
      "99\n",
      "37 99 tensor(-15.4079)\n",
      "Episode: 37, step: 99, total_reward: -10.000000, epsilon: 0.682555\n",
      "99\n",
      "38 99 tensor(-88.6204)\n",
      "Episode: 38, step: 99, total_reward: -10.000000, epsilon: 0.675729\n",
      "52\n",
      "39 52 tensor(-66.7254)\n",
      "Episode: 39, step: 52, total_reward: -6.200000, epsilon: 0.668972\n",
      "99\n",
      "40 99 tensor(-45.4141)\n",
      "Episode: 40, step: 99, total_reward: -10.000000, epsilon: 0.662282\n",
      "99\n",
      "41 99 tensor(-40.4981)\n",
      "Episode: 41, step: 99, total_reward: -10.000000, epsilon: 0.655659\n",
      "40\n",
      "42 40 tensor(-30.1457)\n",
      "Episode: 42, step: 40, total_reward: -3.000000, epsilon: 0.649103\n",
      "[0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 13, 14, 14]\n",
      "99\n",
      "43 99 tensor(-58.3243)\n",
      "Episode: 43, step: 99, total_reward: -10.000000, epsilon: 0.642612\n",
      "48\n",
      "44 48 tensor(-55.5944)\n",
      "Episode: 44, step: 48, total_reward: -5.800000, epsilon: 0.636185\n",
      "2\n",
      "45 2 tensor(-5.6072)\n",
      "Episode: 45, step: 2, total_reward: -1.200000, epsilon: 0.629824\n",
      "99\n",
      "46 99 tensor(-17.9378)\n",
      "Episode: 46, step: 99, total_reward: -10.000000, epsilon: 0.623525\n",
      "90\n",
      "47 90 tensor(-60.1521)\n",
      "Episode: 47, step: 90, total_reward: -10.000000, epsilon: 0.617290\n",
      "99\n",
      "48 99 tensor(-57.3693)\n",
      "Episode: 48, step: 99, total_reward: -10.000000, epsilon: 0.611117\n",
      "99\n",
      "49 99 tensor(-92.4669)\n",
      "Episode: 49, step: 99, total_reward: -10.000000, epsilon: 0.605006\n",
      "99\n",
      "50 99 tensor(-9.3948)\n",
      "Episode: 50, step: 99, total_reward: -10.000000, epsilon: 0.598956\n",
      "1\n",
      "51 1 tensor(-5.1539)\n",
      "Episode: 51, step: 1, total_reward: -1.100000, epsilon: 0.592966\n",
      "99\n",
      "52 99 tensor(-60.0476)\n",
      "Episode: 52, step: 99, total_reward: -10.000000, epsilon: 0.587037\n",
      "99\n",
      "53 99 tensor(-76.8454)\n",
      "Episode: 53, step: 99, total_reward: -10.000000, epsilon: 0.581166\n",
      "10\n",
      "54 10 tensor(-8.8488)\n",
      "Episode: 54, step: 10, total_reward: 0.000000, epsilon: 0.575355\n",
      "[0, 4, 4, 8, 8, 8, 9, 13, 13, 14, 14, 14]\n",
      "41\n",
      "55 41 tensor(-50.7237)\n",
      "Episode: 55, step: 41, total_reward: -3.100000, epsilon: 0.569601\n",
      "[0, 0, 1, 0, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 13, 13, 14, 13, 13, 14, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14]\n",
      "93\n",
      "56 93 tensor(-88.4215)\n",
      "Episode: 56, step: 93, total_reward: -10.300000, epsilon: 0.563905\n",
      "99\n",
      "57 99 tensor(-62.6078)\n",
      "Episode: 57, step: 99, total_reward: -10.000000, epsilon: 0.558266\n",
      "99\n",
      "58 99 tensor(-8.3030)\n",
      "Episode: 58, step: 99, total_reward: -10.000000, epsilon: 0.552683\n",
      "99\n",
      "59 99 tensor(-45.4114)\n",
      "Episode: 59, step: 99, total_reward: -10.000000, epsilon: 0.547157\n",
      "13\n",
      "60 13 tensor(-18.6137)\n",
      "Episode: 60, step: 13, total_reward: -2.300000, epsilon: 0.541685\n",
      "99\n",
      "61 99 tensor(-53.8108)\n",
      "Episode: 61, step: 99, total_reward: -10.000000, epsilon: 0.536268\n",
      "99\n",
      "62 99 tensor(-66.7885)\n",
      "Episode: 62, step: 99, total_reward: -10.000000, epsilon: 0.530906\n",
      "7\n",
      "63 7 tensor(-9.8643)\n",
      "Episode: 63, step: 7, total_reward: -1.700000, epsilon: 0.525596\n",
      "99\n",
      "64 99 tensor(-64.7933)\n",
      "Episode: 64, step: 99, total_reward: -10.000000, epsilon: 0.520341\n",
      "99\n",
      "65 99 tensor(-61.4501)\n",
      "Episode: 65, step: 99, total_reward: -10.000000, epsilon: 0.515137\n",
      "99\n",
      "66 99 tensor(-72.5078)\n",
      "Episode: 66, step: 99, total_reward: -10.000000, epsilon: 0.509986\n",
      "65\n",
      "67 65 tensor(-79.5593)\n",
      "Episode: 67, step: 65, total_reward: -5.500000, epsilon: 0.504886\n",
      "[0, 0, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 10, 9, 13, 13, 13, 13, 13, 13, 14, 14]\n",
      "13\n",
      "68 13 tensor(-15.3362)\n",
      "Episode: 68, step: 13, total_reward: -2.300000, epsilon: 0.499837\n",
      "99\n",
      "69 99 tensor(-18.3425)\n",
      "Episode: 69, step: 99, total_reward: -10.000000, epsilon: 0.494839\n",
      "99\n",
      "70 99 tensor(-102.7910)\n",
      "Episode: 70, step: 99, total_reward: -10.000000, epsilon: 0.489890\n",
      "99\n",
      "71 99 tensor(-10.5080)\n",
      "Episode: 71, step: 99, total_reward: -10.000000, epsilon: 0.484991\n",
      "99\n",
      "72 99 tensor(-80.7717)\n",
      "Episode: 72, step: 99, total_reward: -10.000000, epsilon: 0.480141\n",
      "99\n",
      "73 99 tensor(-83.8264)\n",
      "Episode: 73, step: 99, total_reward: -10.000000, epsilon: 0.475340\n",
      "99\n",
      "74 99 tensor(-27.1783)\n",
      "Episode: 74, step: 99, total_reward: -10.000000, epsilon: 0.470587\n",
      "99\n",
      "75 99 tensor(-40.4901)\n",
      "Episode: 75, step: 99, total_reward: -10.000000, epsilon: 0.465881\n",
      "99\n",
      "76 99 tensor(-34.8085)\n",
      "Episode: 76, step: 99, total_reward: -10.000000, epsilon: 0.461222\n",
      "99\n",
      "77 99 tensor(-67.7204)\n",
      "Episode: 77, step: 99, total_reward: -10.000000, epsilon: 0.456610\n",
      "99\n",
      "78 99 tensor(-12.7321)\n",
      "Episode: 78, step: 99, total_reward: -10.000000, epsilon: 0.452044\n",
      "99\n",
      "79 99 tensor(-50.7662)\n",
      "Episode: 79, step: 99, total_reward: -10.000000, epsilon: 0.447523\n",
      "52\n",
      "80 52 tensor(-36.8348)\n",
      "Episode: 80, step: 52, total_reward: -6.200000, epsilon: 0.443048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "81 99 tensor(-91.0104)\n",
      "Episode: 81, step: 99, total_reward: -10.000000, epsilon: 0.438618\n",
      "76\n",
      "82 76 tensor(-54.2911)\n",
      "Episode: 82, step: 76, total_reward: -8.600000, epsilon: 0.434231\n",
      "99\n",
      "83 99 tensor(-24.1562)\n",
      "Episode: 83, step: 99, total_reward: -10.000000, epsilon: 0.429889\n",
      "99\n",
      "84 99 tensor(-16.2114)\n",
      "Episode: 84, step: 99, total_reward: -10.000000, epsilon: 0.425590\n",
      "99\n",
      "85 99 tensor(-33.4939)\n",
      "Episode: 85, step: 99, total_reward: -10.000000, epsilon: 0.421334\n",
      "28\n",
      "86 28 tensor(-32.9313)\n",
      "Episode: 86, step: 28, total_reward: -3.800000, epsilon: 0.417121\n",
      "99\n",
      "87 99 tensor(-82.1631)\n",
      "Episode: 87, step: 99, total_reward: -10.000000, epsilon: 0.412950\n",
      "99\n",
      "88 99 tensor(-121.9991)\n",
      "Episode: 88, step: 99, total_reward: -10.000000, epsilon: 0.408820\n",
      "99\n",
      "89 99 tensor(-111.3214)\n",
      "Episode: 89, step: 99, total_reward: -10.000000, epsilon: 0.404732\n",
      "99\n",
      "90 99 tensor(-33.8744)\n",
      "Episode: 90, step: 99, total_reward: -10.000000, epsilon: 0.400685\n",
      "99\n",
      "91 99 tensor(-128.5864)\n",
      "Episode: 91, step: 99, total_reward: -10.000000, epsilon: 0.396678\n",
      "97\n",
      "92 97 tensor(-97.5910)\n",
      "Episode: 92, step: 97, total_reward: -10.700000, epsilon: 0.392711\n",
      "99\n",
      "93 99 tensor(-68.3203)\n",
      "Episode: 93, step: 99, total_reward: -10.000000, epsilon: 0.388784\n",
      "99\n",
      "94 99 tensor(-58.6550)\n",
      "Episode: 94, step: 99, total_reward: -10.000000, epsilon: 0.384896\n",
      "14\n",
      "95 14 tensor(-18.9674)\n",
      "Episode: 95, step: 14, total_reward: -2.400000, epsilon: 0.381047\n",
      "99\n",
      "96 99 tensor(-113.8630)\n",
      "Episode: 96, step: 99, total_reward: -10.000000, epsilon: 0.377237\n",
      "99\n",
      "97 99 tensor(-50.7901)\n",
      "Episode: 97, step: 99, total_reward: -10.000000, epsilon: 0.373464\n",
      "99\n",
      "98 99 tensor(-7.2009)\n",
      "Episode: 98, step: 99, total_reward: -10.000000, epsilon: 0.369730\n",
      "32\n",
      "99 32 tensor(-41.1693)\n",
      "Episode: 99, step: 32, total_reward: -4.200000, epsilon: 0.366032\n",
      "min step:  10 min step episode 54\n",
      "shortest path:  [0, 4, 4, 8, 8, 8, 9, 13, 13, 14, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "\n",
    "gamma=0.99\n",
    "epsilon0 = 1.0\n",
    "\n",
    "\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "\n",
    "#scores=[]\n",
    "episodes=[]\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "    samples=[]\n",
    "    for step in range(0,max_step):\n",
    "        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m[0][state]=1.0\n",
    "        state_m_torch=torch.from_numpy(state_m)\n",
    "        out_softmax=model(state_m_torch)\n",
    "#        print(state)\n",
    "#        print(out_softmax)\n",
    "        policy=out_softmax.cpu().detach().numpy()[0]\n",
    "        action=np.random.choice(env.action_space.n,p=policy)\n",
    "\n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "#        print(state_next)\n",
    "        if reward==0 : \n",
    "            reward=-0.1\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1.0\n",
    "\n",
    "        samples+=[[state,action,reward]]\n",
    "#        scores+=reward\n",
    "        \n",
    "        state=copy.deepcopy(state_next)\n",
    "        \n",
    "#        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "#        state_m_next[0][state_next]=1.0\n",
    "        \n",
    "        if game_over or step==max_step-1: \n",
    "            print(step)\n",
    "\n",
    "            rr=0\n",
    "            Nsamples=len(samples)\n",
    "            Gt=np.zeros([Nsamples,1],dtype=np.float32)\n",
    "            states_m=np.zeros([Nsamples,env.observation_space.n],dtype=np.float32)\n",
    "#            action_m=np.zeros([Nsamples],dtype=np.long)\n",
    "            actions_m=np.zeros([Nsamples,env.action_space.n],dtype=np.float32)\n",
    "            for i in range(Nsamples):\n",
    "#                print(samples[i])\n",
    "                state=samples[i][0]\n",
    "                action=samples[i][1]\n",
    "                reward=samples[i][2]\n",
    "                rr= rr*gamma+reward\n",
    "                Gt[i]=rr\n",
    "                states_m[i][state]=1.0\n",
    "                actions_m[i][action]=1.0\n",
    "#                actions_m[i]=action\n",
    "\n",
    "            states_m_torch=torch.from_numpy(states_m)\n",
    "            actions_m_torch=torch.from_numpy(actions_m)\n",
    "            Gt_torch=torch.from_numpy(Gt)\n",
    "\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            out_softmax=model(states_m_torch)\n",
    "            loss=-torch.sum(actions_m_torch*torch.log(out_softmax)*Gt_torch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            cost=loss.data\n",
    "            print(episode,step,cost)\n",
    "\n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "\n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      " 0.348  0.215  0.204  0.233 \n",
      " 0.380  0.191  0.196  0.233 \n",
      " 0.352  0.221  0.201  0.226 \n",
      " 0.222  0.198  0.200  0.380 \n",
      " 0.260  0.222  0.200  0.318 \n",
      " 0.223  0.211  0.194  0.373 \n",
      " 0.381  0.203  0.199  0.217 \n",
      " 0.331  0.201  0.194  0.274 \n",
      " 0.357  0.200  0.195  0.249 \n",
      " 0.272  0.200  0.209  0.318 \n",
      " 0.358  0.220  0.195  0.228 \n",
      " 0.256  0.204  0.204  0.336 \n",
      " 0.313  0.210  0.199  0.278 \n",
      " 0.288  0.210  0.209  0.292 \n",
      " 0.242  0.273  0.211  0.273 \n",
      " 0.262  0.201  0.196  0.340 \n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([env.observation_space.n,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,env.observation_space.n):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "state_m_torch=torch.from_numpy(state_m)\n",
    "Q_pred=torch.softmax(model(state_m_torch),dim=1)\n",
    "Q_pred=Q_pred.cpu().detach().numpy()\n",
    "#print(Q_pred)\n",
    "for i in range(0,env.observation_space.n):\n",
    "    line=\"\"\n",
    "    for j in range(0,env.action_space.n):\n",
    "        line+=\"%6.3f \" %(Q_pred[i][j])\n",
    "#    line+=\"\\n\"\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
