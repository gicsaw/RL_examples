{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "#env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4\n"
     ]
    }
   ],
   "source": [
    "inputdim=env.observation_space.n\n",
    "outputdim=env.action_space.n\n",
    "hiddendim=100\n",
    "print(inputdim, outputdim)\n",
    "X=tf.placeholder(tf.float32, shape = [None,inputdim],name=\"X\")\n",
    "Y=tf.placeholder(tf.float32, shape = [None,outputdim],name=\"Y\")\n",
    "# Y=tf.placeholder(tf.float32,shape=[None,1],name=\"Y\")\n",
    "# action=tf.placeholder(tf.int32,shape=[1],name=\"action\")\n",
    "\n",
    "weight1=tf.get_variable(\"w1\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight2=tf.get_variable(\"w2\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight3=tf.get_variable(\"w3\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight4=tf.get_variable(\"w4\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist=[weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]\n",
    "varlist=[weight1,weight4,bias1,bias4]\n",
    "H1=tf.nn.relu(tf.nn.xw_plus_b(X,weight1,bias1))\n",
    "#H2=tf.nn.relu(tf.nn.xw_plus_b(H1,weight2,bias2))\n",
    "#H3=tf.nn.relu(tf.nn.xw_plus_b(H2,weight3,bias3))\n",
    "#Y_pred=tf.nn.xw_plus_b(H3,weight4,bias4)\n",
    "Y_pred=tf.nn.xw_plus_b(H1,weight4,bias4)\n",
    "\n",
    "\n",
    "# Y=tf.placeholder(tf.float32,shape=[None,1],name=\"Y\")\n",
    "# action=tf.placeholder(tf.int32,shape=[1],name=\"action\")\n",
    "\n",
    "weight01=tf.get_variable(\"w01\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias01=tf.get_variable(\"b01\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight02=tf.get_variable(\"w02\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias02=tf.get_variable(\"b02\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight03=tf.get_variable(\"w03\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias03=tf.get_variable(\"b03\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight04=tf.get_variable(\"w04\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias04=tf.get_variable(\"b04\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist0=[weight01,weight02,weight03,weight04,bias01,bias02,bias03,bias04]\n",
    "#varlist=[weight01,weight04,bias01,bias04]\n",
    "H01=tf.nn.relu(tf.nn.xw_plus_b(X,weight01,bias01))\n",
    "#H02=tf.nn.relu(tf.nn.xw_plus_b(H01,weight02,bias02))\n",
    "#H03=tf.nn.relu(tf.nn.xw_plus_b(H02,weight03,bias03))\n",
    "#Y_pred0=tf.nn.xw_plus_b(H03,weight04,bias04)\n",
    "Y_pred0=tf.nn.xw_plus_b(H01,weight04,bias04)\n",
    "\n",
    "assw1=weight01.assign(weight1)\n",
    "assw2=weight02.assign(weight2)\n",
    "assw3=weight03.assign(weight3)\n",
    "assw4=weight04.assign(weight4)\n",
    "assb1=bias01.assign(bias1)\n",
    "assb2=bias02.assign(bias2)\n",
    "assb3=bias03.assign(bias3)\n",
    "assb4=bias04.assign(bias4)\n",
    "\n",
    "#loss=tf.square(Y-Y_pred)\n",
    "loss=tf.reduce_mean(tf.square(Y-Y_pred))\n",
    "# loss=tf.square(Y-Y_pred[action])\n",
    "#opt=tf.train.AdadeltaOptimizer(1.0).minimize(loss,var_list=varlist)\n",
    "#opt=tf.train.GradientDescentOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "#opt=tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, step: 13, total_reward: -1.120000, epsilon: 0.495000\n",
      "Episode: 1, step: 11, total_reward: -1.100000, epsilon: 0.490050\n",
      "Episode: 2, step: 10, total_reward: -1.090000, epsilon: 0.485149\n",
      "Episode: 3, step: 12, total_reward: -1.110000, epsilon: 0.480298\n",
      "Episode: 4, step: 31, total_reward: -1.300000, epsilon: 0.475495\n",
      "Episode: 5, step: 8, total_reward: -1.070000, epsilon: 0.470740\n",
      "100 [0.12620485] [0.05367122]\n",
      "Episode: 6, step: 23, total_reward: -1.220000, epsilon: 0.466033\n",
      "Episode: 7, step: 24, total_reward: -1.230000, epsilon: 0.461372\n",
      "Episode: 8, step: 22, total_reward: -1.210000, epsilon: 0.456759\n",
      "Episode: 9, step: 11, total_reward: -1.100000, epsilon: 0.452191\n",
      "Episode: 10, step: 4, total_reward: -1.030000, epsilon: 0.447669\n",
      "Episode: 11, step: 21, total_reward: -1.200000, epsilon: 0.443192\n",
      "200 [0.070781566] [0.06756154]\n",
      "Episode: 12, step: 14, total_reward: -1.130000, epsilon: 0.438761\n",
      "Episode: 13, step: 8, total_reward: 0.930000, epsilon: 0.434373\n",
      "[0, 0, 0, 1, 2, 6, 10, 14, 15]\n",
      "Episode: 14, step: 42, total_reward: -1.410000, epsilon: 0.430029\n",
      "Episode: 15, step: 43, total_reward: -1.420000, epsilon: 0.425729\n",
      "300 [0.059343785] [0.05526464]\n",
      "300 update\n",
      "Episode: 16, step: 11, total_reward: -1.100000, epsilon: 0.421472\n",
      "Episode: 17, step: 2, total_reward: -1.010000, epsilon: 0.417257\n",
      "Episode: 18, step: 14, total_reward: -1.130000, epsilon: 0.413084\n",
      "Episode: 19, step: 16, total_reward: -1.150000, epsilon: 0.408953\n",
      "Episode: 20, step: 34, total_reward: -1.330000, epsilon: 0.404864\n",
      "Episode: 21, step: 7, total_reward: -1.060000, epsilon: 0.400815\n",
      "Episode: 22, step: 17, total_reward: -1.160000, epsilon: 0.396807\n",
      "400 [0.07641246] [0.061846543]\n",
      "Episode: 23, step: 17, total_reward: -1.160000, epsilon: 0.392839\n",
      "Episode: 24, step: 22, total_reward: -1.210000, epsilon: 0.388911\n",
      "Episode: 25, step: 13, total_reward: -1.120000, epsilon: 0.385022\n",
      "Episode: 26, step: 7, total_reward: 0.940000, epsilon: 0.381171\n",
      "[0, 0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 27, step: 19, total_reward: -1.180000, epsilon: 0.377360\n",
      "500 [0.051278125] [0.04076479]\n",
      "Episode: 28, step: 44, total_reward: -1.430000, epsilon: 0.373586\n",
      "Episode: 29, step: 18, total_reward: -1.170000, epsilon: 0.369850\n",
      "Episode: 30, step: 22, total_reward: -1.210000, epsilon: 0.366152\n",
      "Episode: 31, step: 24, total_reward: -1.230000, epsilon: 0.362490\n",
      "Episode: 32, step: 6, total_reward: -1.050000, epsilon: 0.358865\n",
      "600 [0.05243627] [0.04960455]\n",
      "600 update\n",
      "Episode: 33, step: 23, total_reward: -1.220000, epsilon: 0.355277\n",
      "Episode: 34, step: 7, total_reward: -1.060000, epsilon: 0.351724\n",
      "Episode: 35, step: 15, total_reward: -1.140000, epsilon: 0.348207\n",
      "Episode: 36, step: 4, total_reward: -1.030000, epsilon: 0.344725\n",
      "Episode: 37, step: 8, total_reward: 0.930000, epsilon: 0.341277\n",
      "[0, 4, 8, 9, 8, 9, 13, 14, 15]\n",
      "700 [0.052087832] [0.047161244]\n",
      "Episode: 38, step: 62, total_reward: -1.610000, epsilon: 0.337865\n",
      "Episode: 39, step: 21, total_reward: -1.200000, epsilon: 0.334486\n",
      "Episode: 40, step: 25, total_reward: -1.240000, epsilon: 0.331141\n",
      "Episode: 41, step: 14, total_reward: -1.130000, epsilon: 0.327830\n",
      "Episode: 42, step: 20, total_reward: -1.190000, epsilon: 0.324551\n",
      "Episode: 43, step: 6, total_reward: -1.050000, epsilon: 0.321306\n",
      "800 [0.049372982] [0.043042477]\n",
      "Episode: 44, step: 21, total_reward: -1.200000, epsilon: 0.318093\n",
      "Episode: 45, step: 18, total_reward: -1.170000, epsilon: 0.314912\n",
      "Episode: 46, step: 13, total_reward: -1.120000, epsilon: 0.311763\n",
      "Episode: 47, step: 36, total_reward: -1.350000, epsilon: 0.308645\n",
      "900 [0.042592525] [0.03799067]\n",
      "900 update\n",
      "Episode: 48, step: 35, total_reward: -1.340000, epsilon: 0.305559\n",
      "Episode: 49, step: 71, total_reward: -1.700000, epsilon: 0.302503\n",
      "Episode: 50, step: 2, total_reward: -1.010000, epsilon: 0.299478\n",
      "Episode: 51, step: 3, total_reward: -1.020000, epsilon: 0.296483\n",
      "Episode: 52, step: 5, total_reward: -1.040000, epsilon: 0.293518\n",
      "1000 [0.046277832] [0.0431211]\n",
      "Episode: 53, step: 18, total_reward: -1.170000, epsilon: 0.290583\n",
      "Episode: 54, step: 25, total_reward: -1.240000, epsilon: 0.287677\n",
      "1100 [0.0505404] [0.046578873]\n",
      "Episode: 55, step: 59, total_reward: -1.580000, epsilon: 0.284801\n",
      "Episode: 56, step: 87, total_reward: 0.140000, epsilon: 0.281953\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 4, 8, 4, 0, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 6, 10, 9, 13, 14, 15]\n",
      "1200 [0.041885864] [0.039215628]\n",
      "1200 update\n",
      "Episode: 57, step: 23, total_reward: -1.220000, epsilon: 0.279133\n",
      "Episode: 58, step: 23, total_reward: -1.220000, epsilon: 0.276342\n",
      "Episode: 59, step: 10, total_reward: -1.090000, epsilon: 0.273578\n",
      "Episode: 60, step: 2, total_reward: -1.010000, epsilon: 0.270843\n",
      "Episode: 61, step: 25, total_reward: -1.240000, epsilon: 0.268134\n",
      "Episode: 62, step: 2, total_reward: -1.010000, epsilon: 0.265453\n",
      "Episode: 63, step: 13, total_reward: -1.120000, epsilon: 0.262798\n",
      "Episode: 64, step: 10, total_reward: -1.090000, epsilon: 0.260170\n",
      "Episode: 65, step: 3, total_reward: -1.020000, epsilon: 0.257569\n",
      "1300 [0.03691399] [0.035535228]\n",
      "Episode: 66, step: 14, total_reward: -1.130000, epsilon: 0.254993\n",
      "Episode: 67, step: 10, total_reward: -1.090000, epsilon: 0.252443\n",
      "Episode: 68, step: 6, total_reward: -1.050000, epsilon: 0.249919\n",
      "Episode: 69, step: 26, total_reward: -1.250000, epsilon: 0.247419\n",
      "Episode: 70, step: 30, total_reward: -1.290000, epsilon: 0.244945\n",
      "1400 [0.040647093] [0.034795836]\n",
      "Episode: 71, step: 27, total_reward: -1.260000, epsilon: 0.242496\n",
      "Episode: 72, step: 18, total_reward: -1.170000, epsilon: 0.240071\n",
      "Episode: 73, step: 6, total_reward: -1.050000, epsilon: 0.237670\n",
      "Episode: 74, step: 55, total_reward: -1.540000, epsilon: 0.235293\n",
      "1500 [0.027517695] [0.027031386]\n",
      "1500 update\n",
      "Episode: 75, step: 36, total_reward: -1.350000, epsilon: 0.232940\n",
      "Episode: 76, step: 24, total_reward: -1.230000, epsilon: 0.230611\n",
      "Episode: 77, step: 6, total_reward: -1.050000, epsilon: 0.228305\n",
      "Episode: 78, step: 9, total_reward: -1.080000, epsilon: 0.226022\n",
      "Episode: 79, step: 17, total_reward: -1.160000, epsilon: 0.223762\n",
      "1600 [0.041307725] [0.038733028]\n",
      "Episode: 80, step: 39, total_reward: -1.380000, epsilon: 0.221524\n",
      "Episode: 81, step: 11, total_reward: -1.100000, epsilon: 0.219309\n",
      "Episode: 82, step: 9, total_reward: -1.080000, epsilon: 0.217116\n",
      "Episode: 83, step: 8, total_reward: -1.070000, epsilon: 0.214945\n",
      "1700 [0.023114] [0.017496103]\n",
      "Episode: 84, step: 51, total_reward: -1.500000, epsilon: 0.212795\n",
      "Episode: 85, step: 79, total_reward: -1.780000, epsilon: 0.210667\n",
      "1800 [0.03155934] [0.029085055]\n",
      "1800 update\n",
      "Episode: 86, step: 30, total_reward: -1.290000, epsilon: 0.208560\n",
      "Episode: 87, step: 33, total_reward: -1.320000, epsilon: 0.206475\n",
      "Episode: 88, step: 38, total_reward: -1.370000, epsilon: 0.204410\n",
      "1900 [0.023194991] [0.021085039]\n",
      "Episode: 89, step: 100, total_reward: -1.000000, epsilon: 0.202366\n",
      "2000 [0.014655894] [0.012352252]\n",
      "Episode: 90, step: 66, total_reward: -1.650000, epsilon: 0.200342\n",
      "2100 [0.025124397] [0.022687418]\n",
      "2100 update\n",
      "Episode: 91, step: 84, total_reward: -1.830000, epsilon: 0.198339\n",
      "Episode: 92, step: 28, total_reward: -1.270000, epsilon: 0.196356\n",
      "Episode: 93, step: 13, total_reward: -1.120000, epsilon: 0.194392\n",
      "Episode: 94, step: 8, total_reward: -1.070000, epsilon: 0.192448\n",
      "Episode: 95, step: 16, total_reward: -1.150000, epsilon: 0.190524\n",
      "2200 [0.01862539] [0.018078253]\n",
      "Episode: 96, step: 8, total_reward: -1.070000, epsilon: 0.188618\n",
      "Episode: 97, step: 21, total_reward: -1.200000, epsilon: 0.186732\n",
      "Episode: 98, step: 13, total_reward: -1.120000, epsilon: 0.184865\n",
      "Episode: 99, step: 36, total_reward: -1.350000, epsilon: 0.183016\n",
      "Episode: 100, step: 10, total_reward: -1.090000, epsilon: 0.181186\n",
      "2300 [0.018241622] [0.015929906]\n",
      "Episode: 101, step: 26, total_reward: -1.250000, epsilon: 0.179374\n",
      "Episode: 102, step: 3, total_reward: -1.020000, epsilon: 0.177580\n",
      "Episode: 103, step: 17, total_reward: -1.160000, epsilon: 0.175805\n",
      "Episode: 104, step: 2, total_reward: -1.010000, epsilon: 0.174047\n",
      "Episode: 105, step: 60, total_reward: -1.590000, epsilon: 0.172306\n",
      "Episode: 106, step: 4, total_reward: -1.030000, epsilon: 0.170583\n",
      "Episode: 107, step: 3, total_reward: -1.020000, epsilon: 0.168877\n",
      "2400 [0.024573078] [0.023050113]\n",
      "2400 update\n",
      "Episode: 108, step: 6, total_reward: -1.050000, epsilon: 0.167188\n",
      "Episode: 109, step: 5, total_reward: -1.040000, epsilon: 0.165517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 110, step: 29, total_reward: -1.280000, epsilon: 0.163861\n",
      "Episode: 111, step: 7, total_reward: -1.060000, epsilon: 0.162223\n",
      "2500 [0.015939679] [0.013319025]\n",
      "Episode: 112, step: 72, total_reward: -1.710000, epsilon: 0.160601\n",
      "Episode: 113, step: 54, total_reward: -1.530000, epsilon: 0.158995\n",
      "Episode: 114, step: 27, total_reward: -1.260000, epsilon: 0.157405\n",
      "2600 [0.01586553] [0.013343914]\n",
      "Episode: 115, step: 24, total_reward: -1.230000, epsilon: 0.155831\n",
      "Episode: 116, step: 34, total_reward: -1.330000, epsilon: 0.154272\n",
      "Episode: 117, step: 30, total_reward: -1.290000, epsilon: 0.152730\n",
      "2700 [0.012928121] [0.012597285]\n",
      "2700 update\n",
      "Episode: 118, step: 16, total_reward: -1.150000, epsilon: 0.151202\n",
      "Episode: 119, step: 17, total_reward: -1.160000, epsilon: 0.149690\n",
      "Episode: 120, step: 57, total_reward: -1.560000, epsilon: 0.148193\n",
      "Episode: 121, step: 2, total_reward: -1.010000, epsilon: 0.146711\n",
      "2800 [0.023890013] [0.022146253]\n",
      "Episode: 122, step: 38, total_reward: -1.370000, epsilon: 0.145244\n",
      "Episode: 123, step: 12, total_reward: -1.110000, epsilon: 0.143792\n",
      "Episode: 124, step: 4, total_reward: -1.030000, epsilon: 0.142354\n",
      "Episode: 125, step: 24, total_reward: -1.230000, epsilon: 0.140930\n",
      "Episode: 126, step: 2, total_reward: -1.010000, epsilon: 0.139521\n",
      "Episode: 127, step: 13, total_reward: -1.120000, epsilon: 0.138126\n",
      "2900 [0.018866632] [0.016572064]\n",
      "Episode: 128, step: 38, total_reward: -1.370000, epsilon: 0.136745\n",
      "Episode: 129, step: 20, total_reward: -1.190000, epsilon: 0.135377\n",
      "3000 [0.011888554] [0.01061985]\n",
      "3000 update\n",
      "Episode: 130, step: 100, total_reward: -1.000000, epsilon: 0.134023\n",
      "Episode: 131, step: 30, total_reward: -1.290000, epsilon: 0.132683\n",
      "3100 [0.011691754] [0.010799425]\n",
      "Episode: 132, step: 100, total_reward: -1.000000, epsilon: 0.131356\n",
      "3200 [0.008022769] [0.00756189]\n",
      "Episode: 133, step: 90, total_reward: -1.890000, epsilon: 0.130043\n",
      "3300 [0.01671443] [0.015876355]\n",
      "3300 update\n",
      "Episode: 134, step: 63, total_reward: -1.620000, epsilon: 0.128742\n",
      "Episode: 135, step: 9, total_reward: -1.080000, epsilon: 0.127455\n",
      "Episode: 136, step: 14, total_reward: -1.130000, epsilon: 0.126180\n",
      "3400 [0.008507587] [0.007884037]\n",
      "Episode: 137, step: 74, total_reward: -1.730000, epsilon: 0.124919\n",
      "Episode: 138, step: 67, total_reward: -1.660000, epsilon: 0.123669\n",
      "3500 [0.01624711] [0.01479438]\n",
      "Episode: 139, step: 80, total_reward: -1.790000, epsilon: 0.122433\n",
      "3600 [0.007763001] [0.006408887]\n",
      "3600 update\n",
      "Episode: 140, step: 75, total_reward: -1.740000, epsilon: 0.121208\n",
      "Episode: 141, step: 20, total_reward: -1.190000, epsilon: 0.119996\n",
      "3700 [0.009414288] [0.008883011]\n",
      "Episode: 142, step: 100, total_reward: -1.000000, epsilon: 0.118796\n",
      "Episode: 143, step: 2, total_reward: -1.010000, epsilon: 0.117608\n",
      "Episode: 144, step: 40, total_reward: -1.390000, epsilon: 0.116432\n",
      "3800 [0.006403718] [0.0059423298]\n",
      "Episode: 145, step: 6, total_reward: -1.050000, epsilon: 0.115268\n",
      "Episode: 146, step: 48, total_reward: -1.470000, epsilon: 0.114115\n",
      "Episode: 147, step: 49, total_reward: -1.480000, epsilon: 0.112974\n",
      "3900 [0.005393237] [0.005457542]\n",
      "3900 update\n",
      "Episode: 148, step: 33, total_reward: -1.320000, epsilon: 0.111844\n",
      "Episode: 149, step: 21, total_reward: -1.200000, epsilon: 0.110726\n",
      "Episode: 150, step: 18, total_reward: -1.170000, epsilon: 0.109619\n",
      "4000 [0.005158311] [0.005126468]\n",
      "Episode: 151, step: 70, total_reward: -1.690000, epsilon: 0.108522\n",
      "4100 [0.0059629856] [0.005864435]\n",
      "Episode: 152, step: 68, total_reward: -1.670000, epsilon: 0.107437\n",
      "Episode: 153, step: 5, total_reward: -1.040000, epsilon: 0.106363\n",
      "Episode: 154, step: 36, total_reward: -1.350000, epsilon: 0.105299\n",
      "Episode: 155, step: 27, total_reward: -1.260000, epsilon: 0.104246\n",
      "Episode: 156, step: 18, total_reward: -1.170000, epsilon: 0.103204\n",
      "4200 [0.014412717] [0.014190733]\n",
      "4200 update\n",
      "Episode: 157, step: 100, total_reward: -1.000000, epsilon: 0.102172\n",
      "4300 [0.005360384] [0.0047292947]\n",
      "Episode: 158, step: 54, total_reward: -1.530000, epsilon: 0.101150\n",
      "4400 [0.0040396284] [0.003628344]\n",
      "Episode: 159, step: 100, total_reward: -1.000000, epsilon: 0.100139\n",
      "Episode: 160, step: 47, total_reward: -1.460000, epsilon: 0.099137\n",
      "4500 [0.009541908] [0.008971619]\n",
      "4500 update\n",
      "Episode: 161, step: 100, total_reward: -1.000000, epsilon: 0.098146\n",
      "4600 [0.0028501356] [0.002868297]\n",
      "Episode: 162, step: 100, total_reward: -1.000000, epsilon: 0.097164\n",
      "4700 [0.011003968] [0.010137693]\n",
      "Episode: 163, step: 100, total_reward: -1.000000, epsilon: 0.096193\n",
      "4800 [0.006237829] [0.0060427445]\n",
      "4800 update\n",
      "Episode: 164, step: 100, total_reward: -1.000000, epsilon: 0.095231\n",
      "4900 [0.0070741] [0.0066154897]\n",
      "Episode: 165, step: 100, total_reward: -1.000000, epsilon: 0.094278\n",
      "5000 [0.0105955815] [0.008836244]\n",
      "Episode: 166, step: 10, total_reward: -1.090000, epsilon: 0.093336\n",
      "Episode: 167, step: 9, total_reward: -1.080000, epsilon: 0.092402\n",
      "5100 [0.006988089] [0.007422568]\n",
      "5100 update\n",
      "Episode: 168, step: 93, total_reward: -1.920000, epsilon: 0.091478\n",
      "Episode: 169, step: 25, total_reward: -1.240000, epsilon: 0.090563\n",
      "5200 [0.0018407436] [0.0011688243]\n",
      "Episode: 170, step: 82, total_reward: -1.810000, epsilon: 0.089658\n",
      "Episode: 171, step: 5, total_reward: -1.040000, epsilon: 0.088761\n",
      "Episode: 172, step: 2, total_reward: -1.010000, epsilon: 0.087874\n",
      "Episode: 173, step: 18, total_reward: -1.170000, epsilon: 0.086995\n",
      "Episode: 174, step: 4, total_reward: -1.030000, epsilon: 0.086125\n",
      "Episode: 175, step: 12, total_reward: -1.110000, epsilon: 0.085264\n",
      "5300 [0.004060222] [0.003945211]\n",
      "Episode: 176, step: 100, total_reward: -1.000000, epsilon: 0.084411\n",
      "Episode: 177, step: 38, total_reward: -1.370000, epsilon: 0.083567\n",
      "5400 [0.0025631303] [0.0024070328]\n",
      "5400 update\n",
      "Episode: 178, step: 22, total_reward: -1.210000, epsilon: 0.082731\n",
      "Episode: 179, step: 15, total_reward: -1.140000, epsilon: 0.081904\n",
      "Episode: 180, step: 25, total_reward: -1.240000, epsilon: 0.081085\n",
      "5500 [0.0073963534] [0.006907561]\n",
      "Episode: 181, step: 100, total_reward: -1.000000, epsilon: 0.080274\n",
      "Episode: 182, step: 35, total_reward: -1.340000, epsilon: 0.079471\n",
      "5600 [0.011121807] [0.010910351]\n",
      "Episode: 183, step: 19, total_reward: -1.180000, epsilon: 0.078677\n",
      "Episode: 184, step: 62, total_reward: -1.610000, epsilon: 0.077890\n",
      "Episode: 185, step: 8, total_reward: -1.070000, epsilon: 0.077111\n",
      "Episode: 186, step: 7, total_reward: -1.060000, epsilon: 0.076340\n",
      "5700 [0.0047813267] [0.0039818017]\n",
      "5700 update\n",
      "Episode: 187, step: 22, total_reward: -1.210000, epsilon: 0.075576\n",
      "Episode: 188, step: 22, total_reward: -1.210000, epsilon: 0.074821\n",
      "Episode: 189, step: 28, total_reward: -1.270000, epsilon: 0.074072\n",
      "Episode: 190, step: 41, total_reward: -1.400000, epsilon: 0.073332\n",
      "5800 [0.0056799776] [0.0053451]\n",
      "Episode: 191, step: 16, total_reward: -1.150000, epsilon: 0.072598\n",
      "Episode: 192, step: 63, total_reward: -1.620000, epsilon: 0.071872\n",
      "5900 [0.01538828] [0.013873623]\n",
      "Episode: 193, step: 48, total_reward: -1.470000, epsilon: 0.071154\n",
      "Episode: 194, step: 57, total_reward: -1.560000, epsilon: 0.070442\n",
      "6000 [0.013061165] [0.012988673]\n",
      "6000 update\n",
      "Episode: 195, step: 67, total_reward: -1.660000, epsilon: 0.069738\n",
      "6100 [0.007935016] [0.006804013]\n",
      "Episode: 196, step: 100, total_reward: -1.000000, epsilon: 0.069040\n",
      "Episode: 197, step: 14, total_reward: -1.130000, epsilon: 0.068350\n",
      "6200 [0.009944207] [0.009554021]\n",
      "Episode: 198, step: 48, total_reward: -1.470000, epsilon: 0.067667\n",
      "6300 [0.006212022] [0.005301042]\n",
      "6300 update\n",
      "Episode: 199, step: 100, total_reward: -1.000000, epsilon: 0.066990\n",
      "min step:  7 min step episode 26\n",
      "shortest path:  [0, 0, 4, 8, 9, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=200\n",
    "max_step=100\n",
    "gamma=0.995\n",
    "epsilon0 = 0.5\n",
    "sess.run(init)\n",
    "sess.run([assw1,assw2,assw3,assw4,assb1,assb2,assb3,assb4])\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "max_replay=1000\n",
    "replay=deque(maxlen=max_replay)\n",
    "i_step=0\n",
    "batch_size=100\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "    \n",
    "    state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "    state_m[0][state]=1.0\n",
    "    Q_pred=sess.run(Y_pred0,feed_dict={X:state_m})\n",
    "    if np.random.rand()>epsilon :\n",
    "        action=np.argmax(Q_pred[0])\n",
    "    else:\n",
    "        action=np.random.choice(env.action_space.n) \n",
    "    for step in range(1,max_step+1):\n",
    "        \n",
    "        i_step+=1\n",
    "   \n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "        \n",
    "        if reward==0 : \n",
    "            reward=-0.01\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1\n",
    "        \n",
    "        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m_next[0][state_next]=1.0\n",
    "        Q_pred_next=sess.run(Y_pred0, feed_dict={X:state_m_next}) \n",
    "        if np.random.rand()>epsilon :\n",
    "            action_next=np.argmax(Q_pred_next[0])\n",
    "        else:\n",
    "            action_next=np.random.choice(env.action_space.n)\n",
    "        \n",
    "        Y_true=copy.deepcopy(Q_pred[0])\n",
    "        if reward==1.0 and game_over==1:\n",
    "            Y_true[action]= reward\n",
    "        else :\n",
    "            Y_true[action]=reward+gamma* (Q_pred_next[0][action_next])\n",
    "        Y_true=np.reshape(Y_true,[1,env.action_space.n])\n",
    "        replay.append([state_m[0],Y_true])\n",
    "        \n",
    "\n",
    "#            game_over=0\n",
    "        # Q-Learning\n",
    "        if i_step%100==0 and i_step!=0:\n",
    "            mini_batch = random.sample(replay,batch_size)\n",
    "            states=np.zeros([batch_size,inputdim])\n",
    "            Qs=np.zeros([batch_size,outputdim])\n",
    "            for m in range(batch_size):\n",
    "                states[m]=mini_batch[m][0]\n",
    "                Qs[m]=mini_batch[m][1]\n",
    "            cost0= sess.run([loss],feed_dict={X:states,Y:Qs})\n",
    "            for i in range(0,5):\n",
    "                sess.run([opt],feed_dict={X:states,Y:Qs})\n",
    "                cost= sess.run([loss],feed_dict={X:states,Y:Qs})\n",
    "#            Q_pred=sess.run(Y_pred,feed_dict={X:states})\n",
    "            #print(i_step,cost0,cost,Qs[0],Q_pred[0])\n",
    "            print(i_step,cost0,cost)\n",
    "\n",
    "        if i_step%300==0 :\n",
    "            print(i_step,\"update\")\n",
    "            sess.run([assw1,assw2,assw3,assw4,assb1,assb2,assb3,assb4])\n",
    "            \n",
    "        state=state_next\n",
    "        state_m=state_m_next\n",
    "        action=action_next\n",
    "        \n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "            \n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n",
    "#print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[-0.3193047   0.0772663   0.12334345  0.1268539 ]\n",
      " [-0.32311273  0.09331876  0.17124411  0.11340132]\n",
      " [-0.3256859   0.06620821  0.1321699   0.12647435]\n",
      " [-0.32298675  0.05508754  0.17610991  0.10357158]\n",
      " [-0.23784913  0.08147555 -0.21349175  0.16446747]\n",
      " [-0.14482181 -0.00185806  0.23423135 -0.10606202]\n",
      " [-0.31880724  0.08778637  0.1956693   0.17126487]\n",
      " [-0.28401023  0.0964123  -0.13534428 -0.0260882 ]\n",
      " [-0.38892242  0.04145909  0.15882131  0.24629675]\n",
      " [-0.09056944  0.29875803  0.16717696 -0.00319835]\n",
      " [-0.42414215  0.11395315  0.24345335  0.20396113]\n",
      " [-0.20755486  0.10998899  0.24761568  0.14405157]\n",
      " [-0.37244168  0.24509059  0.34299713  0.25176606]\n",
      " [-0.39547226  0.1354162   0.7089015   0.30775604]\n",
      " [-0.34181517  0.12630263  0.7805561   0.2928009 ]\n",
      " [ 0.0197199  -0.00654274  0.02214172 -0.00803148]]\n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([16,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,15):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "print(Q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
