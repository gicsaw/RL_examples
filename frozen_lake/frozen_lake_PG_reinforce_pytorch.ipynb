{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "#env =FrozenLakeEnv(is_slippery=False,map_name=\"8x8\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,para,bias=True):\n",
    "        super(Net,self).__init__()\n",
    "        self.input_dim=para['input_dim']\n",
    "        self.output_dim=para['output_dim']\n",
    "        self.hidden_dim=para['hidden_dim']\n",
    "        \n",
    "        self.w1=Parameter(torch.FloatTensor(self.input_dim,self.hidden_dim))\n",
    "        self.b1=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w1)\n",
    "#        nn.init.kaiming_uniform_(self.w1)\n",
    "        nn.init.uniform_(self.b1,-0.001,0.001)\n",
    "\n",
    "        self.w2=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b2=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w2)\n",
    "#        nn.init.kaiming_uniform_(self.w2)\n",
    "        nn.init.uniform_(self.b2,-0.001,0.001)\n",
    "        \n",
    "        self.w3=Parameter(torch.FloatTensor(self.hidden_dim,self.hidden_dim))\n",
    "        self.b3=Parameter(torch.FloatTensor(self.hidden_dim))\n",
    "        nn.init.kaiming_normal_(self.w3)\n",
    "#        nn.init.kaiming_uniform_(self.w3)\n",
    "        nn.init.uniform_(self.b3,-0.001,0.001)\n",
    "        \n",
    "        self.w4=Parameter(torch.FloatTensor(self.hidden_dim,self.output_dim))\n",
    "        self.b4=Parameter(torch.FloatTensor(self.output_dim))\n",
    "        nn.init.kaiming_normal_(self.w4)\n",
    "#        nn.init.kaiming_uniform_(self.w4)\n",
    "        nn.init.uniform_(self.b4,-0.001,0.001)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        H=torch.relu( torch.einsum('li,ij->lj',X,self.w1)+self.b1)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w2)+self.b2)\n",
    "        H=torch.relu( torch.einsum('li,ij->lj',H,self.w3)+self.b3)\n",
    "        Y=torch.einsum('li,ij->lj',H,self.w4)+self.b4\n",
    "        Y=torch.softmax(Y,dim=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=env.observation_space.n\n",
    "output_dim=env.action_space.n\n",
    "hidden_dim=100\n",
    "lr=0.0001\n",
    "lr2=1.0\n",
    "\n",
    "para={'input_dim': input_dim, 'output_dim': output_dim, 'hidden_dim': hidden_dim}\n",
    "model=Net(para)\n",
    "#criterion=nn.MSELoss()\n",
    "#criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "0 39 tensor(-53.6139)\n",
      "Episode: 0, step: 39, total_reward: -4.900000, epsilon: 0.990000\n",
      "28\n",
      "1 28 tensor(-46.4481)\n",
      "Episode: 1, step: 28, total_reward: -3.800000, epsilon: 0.980100\n",
      "45\n",
      "2 45 tensor(-84.1848)\n",
      "Episode: 2, step: 45, total_reward: -5.500000, epsilon: 0.970299\n",
      "17\n",
      "3 17 tensor(-21.2413)\n",
      "Episode: 3, step: 17, total_reward: -2.700000, epsilon: 0.960596\n",
      "99\n",
      "4 99 tensor(-292.1635)\n",
      "Episode: 4, step: 99, total_reward: -10.000000, epsilon: 0.950990\n",
      "99\n",
      "5 99 tensor(-287.7385)\n",
      "Episode: 5, step: 99, total_reward: -10.000000, epsilon: 0.941480\n",
      "46\n",
      "6 46 tensor(-56.9339)\n",
      "Episode: 6, step: 46, total_reward: -5.600000, epsilon: 0.932065\n",
      "94\n",
      "7 94 tensor(-236.7540)\n",
      "Episode: 7, step: 94, total_reward: -10.400000, epsilon: 0.922745\n",
      "39\n",
      "8 39 tensor(-66.3811)\n",
      "Episode: 8, step: 39, total_reward: -4.900000, epsilon: 0.913517\n",
      "99\n",
      "9 99 tensor(-275.0953)\n",
      "Episode: 9, step: 99, total_reward: -10.000000, epsilon: 0.904382\n",
      "99\n",
      "10 99 tensor(-142.8928)\n",
      "Episode: 10, step: 99, total_reward: -10.000000, epsilon: 0.895338\n",
      "9\n",
      "11 9 tensor(-9.4744)\n",
      "Episode: 11, step: 9, total_reward: -1.900000, epsilon: 0.886385\n",
      "69\n",
      "12 69 tensor(-130.3368)\n",
      "Episode: 12, step: 69, total_reward: -7.900000, epsilon: 0.877521\n",
      "81\n",
      "13 81 tensor(-181.5096)\n",
      "Episode: 13, step: 81, total_reward: -9.100000, epsilon: 0.868746\n",
      "63\n",
      "14 63 tensor(-107.6416)\n",
      "Episode: 14, step: 63, total_reward: -7.300000, epsilon: 0.860058\n",
      "99\n",
      "15 99 tensor(-252.1181)\n",
      "Episode: 15, step: 99, total_reward: -10.000000, epsilon: 0.851458\n",
      "78\n",
      "16 78 tensor(-179.7723)\n",
      "Episode: 16, step: 78, total_reward: -8.800000, epsilon: 0.842943\n",
      "30\n",
      "17 30 tensor(-22.4614)\n",
      "Episode: 17, step: 30, total_reward: -4.000000, epsilon: 0.834514\n",
      "70\n",
      "18 70 tensor(-96.1138)\n",
      "Episode: 18, step: 70, total_reward: -8.000000, epsilon: 0.826169\n",
      "52\n",
      "19 52 tensor(-96.0124)\n",
      "Episode: 19, step: 52, total_reward: -6.200000, epsilon: 0.817907\n",
      "72\n",
      "20 72 tensor(-129.6663)\n",
      "Episode: 20, step: 72, total_reward: -8.200000, epsilon: 0.809728\n",
      "13\n",
      "21 13 tensor(-11.9588)\n",
      "Episode: 21, step: 13, total_reward: -2.300000, epsilon: 0.801631\n",
      "19\n",
      "22 19 tensor(-19.1376)\n",
      "Episode: 22, step: 19, total_reward: -2.900000, epsilon: 0.793614\n",
      "99\n",
      "23 99 tensor(-116.2884)\n",
      "Episode: 23, step: 99, total_reward: -10.000000, epsilon: 0.785678\n",
      "99\n",
      "24 99 tensor(-129.6687)\n",
      "Episode: 24, step: 99, total_reward: -10.000000, epsilon: 0.777821\n",
      "61\n",
      "25 61 tensor(-88.7074)\n",
      "Episode: 25, step: 61, total_reward: -7.100000, epsilon: 0.770043\n",
      "38\n",
      "26 38 tensor(-29.7463)\n",
      "Episode: 26, step: 38, total_reward: -4.800000, epsilon: 0.762343\n",
      "80\n",
      "27 80 tensor(-160.9749)\n",
      "Episode: 27, step: 80, total_reward: -9.000000, epsilon: 0.754719\n",
      "99\n",
      "28 99 tensor(-93.3654)\n",
      "Episode: 28, step: 99, total_reward: -10.000000, epsilon: 0.747172\n",
      "99\n",
      "29 99 tensor(-199.7767)\n",
      "Episode: 29, step: 99, total_reward: -10.000000, epsilon: 0.739700\n",
      "40\n",
      "30 40 tensor(-59.8092)\n",
      "Episode: 30, step: 40, total_reward: -5.000000, epsilon: 0.732303\n",
      "10\n",
      "31 10 tensor(-10.5603)\n",
      "Episode: 31, step: 10, total_reward: -2.000000, epsilon: 0.724980\n",
      "99\n",
      "32 99 tensor(-184.9818)\n",
      "Episode: 32, step: 99, total_reward: -10.000000, epsilon: 0.717731\n",
      "99\n",
      "33 99 tensor(-196.1678)\n",
      "Episode: 33, step: 99, total_reward: -10.000000, epsilon: 0.710553\n",
      "99\n",
      "34 99 tensor(-281.6924)\n",
      "Episode: 34, step: 99, total_reward: -10.000000, epsilon: 0.703448\n",
      "99\n",
      "35 99 tensor(-223.6742)\n",
      "Episode: 35, step: 99, total_reward: -10.000000, epsilon: 0.696413\n",
      "15\n",
      "36 15 tensor(-22.1901)\n",
      "Episode: 36, step: 15, total_reward: -2.500000, epsilon: 0.689449\n",
      "99\n",
      "37 99 tensor(-199.0708)\n",
      "Episode: 37, step: 99, total_reward: -10.000000, epsilon: 0.682555\n",
      "99\n",
      "38 99 tensor(-244.5304)\n",
      "Episode: 38, step: 99, total_reward: -10.000000, epsilon: 0.675729\n",
      "99\n",
      "39 99 tensor(-177.0357)\n",
      "Episode: 39, step: 99, total_reward: -10.000000, epsilon: 0.668972\n",
      "68\n",
      "40 68 tensor(-169.4900)\n",
      "Episode: 40, step: 68, total_reward: -7.800000, epsilon: 0.662282\n",
      "99\n",
      "41 99 tensor(-222.3869)\n",
      "Episode: 41, step: 99, total_reward: -10.000000, epsilon: 0.655659\n",
      "99\n",
      "42 99 tensor(-232.9338)\n",
      "Episode: 42, step: 99, total_reward: -10.000000, epsilon: 0.649103\n",
      "99\n",
      "43 99 tensor(-185.5495)\n",
      "Episode: 43, step: 99, total_reward: -10.000000, epsilon: 0.642612\n",
      "22\n",
      "44 22 tensor(-39.7734)\n",
      "Episode: 44, step: 22, total_reward: -3.200000, epsilon: 0.636185\n",
      "99\n",
      "45 99 tensor(-204.2678)\n",
      "Episode: 45, step: 99, total_reward: -10.000000, epsilon: 0.629824\n",
      "29\n",
      "46 29 tensor(-45.2203)\n",
      "Episode: 46, step: 29, total_reward: -3.900000, epsilon: 0.623525\n",
      "38\n",
      "47 38 tensor(-64.5604)\n",
      "Episode: 47, step: 38, total_reward: -4.800000, epsilon: 0.617290\n",
      "78\n",
      "48 78 tensor(-115.6694)\n",
      "Episode: 48, step: 78, total_reward: -8.800000, epsilon: 0.611117\n",
      "19\n",
      "49 19 tensor(-18.3722)\n",
      "Episode: 49, step: 19, total_reward: -2.900000, epsilon: 0.605006\n",
      "54\n",
      "50 54 tensor(-59.9469)\n",
      "Episode: 50, step: 54, total_reward: -6.400000, epsilon: 0.598956\n",
      "99\n",
      "51 99 tensor(-264.1075)\n",
      "Episode: 51, step: 99, total_reward: -10.000000, epsilon: 0.592966\n",
      "18\n",
      "52 18 tensor(-18.7779)\n",
      "Episode: 52, step: 18, total_reward: -2.800000, epsilon: 0.587037\n",
      "23\n",
      "53 23 tensor(-27.2397)\n",
      "Episode: 53, step: 23, total_reward: -3.300000, epsilon: 0.581166\n",
      "99\n",
      "54 99 tensor(-189.9953)\n",
      "Episode: 54, step: 99, total_reward: -10.000000, epsilon: 0.575355\n",
      "99\n",
      "55 99 tensor(-185.4628)\n",
      "Episode: 55, step: 99, total_reward: -10.000000, epsilon: 0.569601\n",
      "46\n",
      "56 46 tensor(-77.1144)\n",
      "Episode: 56, step: 46, total_reward: -5.600000, epsilon: 0.563905\n",
      "23\n",
      "57 23 tensor(-29.3500)\n",
      "Episode: 57, step: 23, total_reward: -3.300000, epsilon: 0.558266\n",
      "29\n",
      "58 29 tensor(-36.7778)\n",
      "Episode: 58, step: 29, total_reward: -3.900000, epsilon: 0.552683\n",
      "39\n",
      "59 39 tensor(-105.7522)\n",
      "Episode: 59, step: 39, total_reward: -4.900000, epsilon: 0.547157\n",
      "36\n",
      "60 36 tensor(-54.1039)\n",
      "Episode: 60, step: 36, total_reward: -4.600000, epsilon: 0.541685\n",
      "12\n",
      "61 12 tensor(-14.6393)\n",
      "Episode: 61, step: 12, total_reward: -2.200000, epsilon: 0.536268\n",
      "99\n",
      "62 99 tensor(-154.5066)\n",
      "Episode: 62, step: 99, total_reward: -10.000000, epsilon: 0.530906\n",
      "99\n",
      "63 99 tensor(-230.1032)\n",
      "Episode: 63, step: 99, total_reward: -10.000000, epsilon: 0.525596\n",
      "99\n",
      "64 99 tensor(-249.8691)\n",
      "Episode: 64, step: 99, total_reward: -10.000000, epsilon: 0.520341\n",
      "2\n",
      "65 2 tensor(-3.4552)\n",
      "Episode: 65, step: 2, total_reward: -1.200000, epsilon: 0.515137\n",
      "99\n",
      "66 99 tensor(-292.5985)\n",
      "Episode: 66, step: 99, total_reward: -10.000000, epsilon: 0.509986\n",
      "2\n",
      "67 2 tensor(-5.1168)\n",
      "Episode: 67, step: 2, total_reward: -1.200000, epsilon: 0.504886\n",
      "68\n",
      "68 68 tensor(-128.2187)\n",
      "Episode: 68, step: 68, total_reward: -7.800000, epsilon: 0.499837\n",
      "30\n",
      "69 30 tensor(-33.1844)\n",
      "Episode: 69, step: 30, total_reward: -4.000000, epsilon: 0.494839\n",
      "9\n",
      "70 9 tensor(-11.8071)\n",
      "Episode: 70, step: 9, total_reward: -1.900000, epsilon: 0.489890\n",
      "7\n",
      "71 7 tensor(-8.5308)\n",
      "Episode: 71, step: 7, total_reward: -1.700000, epsilon: 0.484991\n",
      "30\n",
      "72 30 tensor(-46.4176)\n",
      "Episode: 72, step: 30, total_reward: -4.000000, epsilon: 0.480141\n",
      "49\n",
      "73 49 tensor(-104.8680)\n",
      "Episode: 73, step: 49, total_reward: -5.900000, epsilon: 0.475340\n",
      "16\n",
      "74 16 tensor(-16.1584)\n",
      "Episode: 74, step: 16, total_reward: -2.600000, epsilon: 0.470587\n",
      "71\n",
      "75 71 tensor(-193.7943)\n",
      "Episode: 75, step: 71, total_reward: -8.100000, epsilon: 0.465881\n",
      "48\n",
      "76 48 tensor(-111.9834)\n",
      "Episode: 76, step: 48, total_reward: -5.800000, epsilon: 0.461222\n",
      "25\n",
      "77 25 tensor(-37.9184)\n",
      "Episode: 77, step: 25, total_reward: -3.500000, epsilon: 0.456610\n",
      "81\n",
      "78 81 tensor(-196.6119)\n",
      "Episode: 78, step: 81, total_reward: -9.100000, epsilon: 0.452044\n",
      "65\n",
      "79 65 tensor(-137.3180)\n",
      "Episode: 79, step: 65, total_reward: -7.500000, epsilon: 0.447523\n",
      "99\n",
      "80 99 tensor(-296.8755)\n",
      "Episode: 80, step: 99, total_reward: -10.000000, epsilon: 0.443048\n",
      "44\n",
      "81 44 tensor(-96.1914)\n",
      "Episode: 81, step: 44, total_reward: -5.400000, epsilon: 0.438618\n",
      "50\n",
      "82 50 tensor(-83.0167)\n",
      "Episode: 82, step: 50, total_reward: -6.000000, epsilon: 0.434231\n",
      "99\n",
      "83 99 tensor(-302.6219)\n",
      "Episode: 83, step: 99, total_reward: -10.000000, epsilon: 0.429889\n",
      "99\n",
      "84 99 tensor(-233.1649)\n",
      "Episode: 84, step: 99, total_reward: -10.000000, epsilon: 0.425590\n",
      "99\n",
      "85 99 tensor(-258.0096)\n",
      "Episode: 85, step: 99, total_reward: -10.000000, epsilon: 0.421334\n",
      "94\n",
      "86 94 tensor(-254.9160)\n",
      "Episode: 86, step: 94, total_reward: -10.400000, epsilon: 0.417121\n",
      "99\n",
      "87 99 tensor(-364.8502)\n",
      "Episode: 87, step: 99, total_reward: -10.000000, epsilon: 0.412950\n",
      "34\n",
      "88 34 tensor(-45.0874)\n",
      "Episode: 88, step: 34, total_reward: -4.400000, epsilon: 0.408820\n",
      "25\n",
      "89 25 tensor(-33.9330)\n",
      "Episode: 89, step: 25, total_reward: -3.500000, epsilon: 0.404732\n",
      "17\n",
      "90 17 tensor(-16.2408)\n",
      "Episode: 90, step: 17, total_reward: -2.700000, epsilon: 0.400685\n",
      "99\n",
      "91 99 tensor(-278.0623)\n",
      "Episode: 91, step: 99, total_reward: -10.000000, epsilon: 0.396678\n",
      "58\n",
      "92 58 tensor(-114.0513)\n",
      "Episode: 92, step: 58, total_reward: -6.800000, epsilon: 0.392711\n",
      "20\n",
      "93 20 tensor(-30.0918)\n",
      "Episode: 93, step: 20, total_reward: -3.000000, epsilon: 0.388784\n",
      "33\n",
      "94 33 tensor(-57.9227)\n",
      "Episode: 94, step: 33, total_reward: -4.300000, epsilon: 0.384896\n",
      "99\n",
      "95 99 tensor(-318.3055)\n",
      "Episode: 95, step: 99, total_reward: -10.000000, epsilon: 0.381047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "96 25 tensor(-41.9502)\n",
      "Episode: 96, step: 25, total_reward: -3.500000, epsilon: 0.377237\n",
      "30\n",
      "97 30 tensor(-39.4309)\n",
      "Episode: 97, step: 30, total_reward: -4.000000, epsilon: 0.373464\n",
      "99\n",
      "98 99 tensor(-317.4319)\n",
      "Episode: 98, step: 99, total_reward: -10.000000, epsilon: 0.369730\n",
      "35\n",
      "99 35 tensor(-57.0469)\n",
      "Episode: 99, step: 35, total_reward: -4.500000, epsilon: 0.366032\n",
      "min step:  100 min step episode 100\n",
      "shortest path:  []\n"
     ]
    }
   ],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "\n",
    "gamma=0.99\n",
    "epsilon0 = 1.0\n",
    "\n",
    "\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "\n",
    "#scores=[]\n",
    "episodes=[]\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "    samples=[]\n",
    "    for step in range(0,max_step):\n",
    "        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m[0][state]=1.0\n",
    "        state_m_torch=torch.from_numpy(state_m)\n",
    "        out_softmax=model(state_m_torch)\n",
    "#        print(state)\n",
    "#        print(out_softmax)\n",
    "        policy=out_softmax.cpu().detach().numpy()[0]\n",
    "        action=np.random.choice(env.action_space.n,p=policy)\n",
    "\n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "#        print(state_next)\n",
    "        if reward==0 : \n",
    "            reward=-0.1\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1.0\n",
    "\n",
    "        samples+=[[state,action,reward]]\n",
    "#        scores+=reward\n",
    "        \n",
    "        state=copy.deepcopy(state_next)\n",
    "        \n",
    "#        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "#        state_m_next[0][state_next]=1.0\n",
    "        \n",
    "        if game_over or step==max_step-1: \n",
    "            print(step)\n",
    "\n",
    "            rr=0\n",
    "            Nsamples=len(samples)\n",
    "            Gt=np.zeros([Nsamples,1],dtype=np.float32)\n",
    "            states_m=np.zeros([Nsamples,env.observation_space.n],dtype=np.float32)\n",
    "#            action_m=np.zeros([Nsamples],dtype=np.long)\n",
    "            actions_m=np.zeros([Nsamples,env.action_space.n],dtype=np.float32)\n",
    "            for i in range(Nsamples):\n",
    "#                print(samples[i])\n",
    "                state=samples[i][0]\n",
    "                action=samples[i][1]\n",
    "                reward=samples[i][2]\n",
    "                rr= rr*gamma+reward\n",
    "                Gt[i]=rr\n",
    "                states_m[i][state]=1.0\n",
    "                actions_m[i][action]=1.0\n",
    "#                actions_m[i]=action\n",
    "\n",
    "            states_m_torch=torch.from_numpy(states_m)\n",
    "            actions_m_torch=torch.from_numpy(actions_m)\n",
    "            Gt_torch=torch.from_numpy(Gt)\n",
    "\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            out_softmax=model(states_m_torch)\n",
    "            loss=-torch.sum(actions_m_torch*torch.log(out_softmax)*Gt_torch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            cost=loss.data\n",
    "            print(episode,step,cost)\n",
    "\n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "\n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      " 0.348  0.215  0.204  0.233 \n",
      " 0.380  0.191  0.196  0.233 \n",
      " 0.352  0.221  0.201  0.226 \n",
      " 0.222  0.198  0.200  0.380 \n",
      " 0.260  0.222  0.200  0.318 \n",
      " 0.223  0.211  0.194  0.373 \n",
      " 0.381  0.203  0.199  0.217 \n",
      " 0.331  0.201  0.194  0.274 \n",
      " 0.357  0.200  0.195  0.249 \n",
      " 0.272  0.200  0.209  0.318 \n",
      " 0.358  0.220  0.195  0.228 \n",
      " 0.256  0.204  0.204  0.336 \n",
      " 0.313  0.210  0.199  0.278 \n",
      " 0.288  0.210  0.209  0.292 \n",
      " 0.242  0.273  0.211  0.273 \n",
      " 0.262  0.201  0.196  0.340 \n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([env.observation_space.n,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,env.observation_space.n):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "state_m_torch=torch.from_numpy(state_m)\n",
    "Q_pred=torch.softmax(model(state_m_torch),dim=1)\n",
    "Q_pred=Q_pred.cpu().detach().numpy()\n",
    "#print(Q_pred)\n",
    "for i in range(0,env.observation_space.n):\n",
    "    line=\"\"\n",
    "    for j in range(0,env.action_space.n):\n",
    "        line+=\"%6.3f \" %(Q_pred[i][j])\n",
    "#    line+=\"\\n\"\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
