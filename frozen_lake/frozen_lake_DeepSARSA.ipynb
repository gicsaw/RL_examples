{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env =FrozenLakeEnv(is_slippery=False,map_name=\"4x4\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "lr=0.02\n",
    "lr2=1.0\n",
    "gamma=0.99\n",
    "epsilon0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4\n"
     ]
    }
   ],
   "source": [
    "inputdim=env.observation_space.n\n",
    "outputdim=env.action_space.n\n",
    "hiddendim=100\n",
    "print(inputdim, outputdim)\n",
    "X=tf.placeholder(tf.float32, shape = [None,inputdim],name=\"X\")\n",
    "Y=tf.placeholder(tf.float32, shape = [None,outputdim],name=\"Y\")\n",
    "\n",
    "#weight1=tf.get_variable(\"w1\", initializer=tf.contrib.layers.xavier_initializer(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "#bias1=tf.get_variable(\"b1\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight2=tf.get_variable(\"w2\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias2=tf.get_variable(\"b2\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight3=tf.get_variable(\"w3\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias3=tf.get_variable(\"b3\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight4=tf.get_variable(\"w4\", initializer=tf.contrib.layers.xavier_initializer(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "#bias4=tf.get_variable(\"b4\", initializer=tf.contrib.layers.xavier_initializer(), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#weight1=tf.get_variable(\"w1\", initializer=tf.initializers.random_normal(0,0.05), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "#bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight2=tf.get_variable(\"w2\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight3=tf.get_variable(\"w3\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "#bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "#weight4=tf.get_variable(\"w4\", initializer=tf.initializers.random_normal(0,0.05), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "#bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "weight1=tf.get_variable(\"w1\", initializer=tf.keras.initializers.he_uniform(), shape=[inputdim,hiddendim], dtype=tf.float32)\n",
    "bias1=tf.get_variable(\"b1\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight2=tf.get_variable(\"w2\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias2=tf.get_variable(\"b2\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight3=tf.get_variable(\"w3\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,hiddendim], dtype=tf.float32)\n",
    "bias3=tf.get_variable(\"b3\", initializer=tf.initializers.random_normal(0,0.00), shape=[hiddendim], dtype=tf.float32)\n",
    "weight4=tf.get_variable(\"w4\", initializer=tf.keras.initializers.he_uniform(), shape=[hiddendim,outputdim], dtype=tf.float32)\n",
    "bias4=tf.get_variable(\"b4\", initializer=tf.initializers.random_normal(0,0.00), shape=[outputdim], dtype=tf.float32)\n",
    "\n",
    "#varlist=[weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]\n",
    "varlist=[weight1,weight4,bias1,bias4]\n",
    "H1=tf.nn.relu(tf.nn.xw_plus_b(X,weight1,bias1))\n",
    "#H2=tf.nn.relu(tf.nn.xw_plus_b(H1,weight2,bias2))\n",
    "#H3=tf.nn.relu(tf.nn.xw_plus_b(H2,weight3,bias3))\n",
    "#Y_pred=tf.nn.xw_plus_b(H3,weight4,bias4)\n",
    "Y_pred=tf.nn.xw_plus_b(H1,weight4,bias4)\n",
    "\n",
    "\n",
    "loss=tf.reduce_sum(tf.square(Y-Y_pred))\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(loss,var_list=varlist)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, step: 6, total_reward: -1.050000, epsilon: 0.099000\n",
      "Episode: 1, step: 32, total_reward: -1.310000, epsilon: 0.098010\n",
      "Episode: 2, step: 14, total_reward: -1.130000, epsilon: 0.097030\n",
      "Episode: 3, step: 80, total_reward: -1.790000, epsilon: 0.096060\n",
      "Episode: 4, step: 42, total_reward: -1.410000, epsilon: 0.095099\n",
      "Episode: 5, step: 4, total_reward: -1.030000, epsilon: 0.094148\n",
      "Episode: 6, step: 4, total_reward: -1.030000, epsilon: 0.093207\n",
      "Episode: 7, step: 10, total_reward: 0.910000, epsilon: 0.092274\n",
      "[0, 4, 8, 9, 13, 13, 13, 13, 13, 14, 15]\n",
      "Episode: 8, step: 2, total_reward: -1.010000, epsilon: 0.091352\n",
      "Episode: 9, step: 3, total_reward: -1.020000, epsilon: 0.090438\n",
      "Episode: 10, step: 2, total_reward: -1.010000, epsilon: 0.089534\n",
      "Episode: 11, step: 36, total_reward: -1.350000, epsilon: 0.088638\n",
      "Episode: 12, step: 85, total_reward: -1.840000, epsilon: 0.087752\n",
      "Episode: 13, step: 100, total_reward: -1.000000, epsilon: 0.086875\n",
      "Episode: 14, step: 15, total_reward: -1.140000, epsilon: 0.086006\n",
      "Episode: 15, step: 100, total_reward: -1.000000, epsilon: 0.085146\n",
      "Episode: 16, step: 100, total_reward: -1.000000, epsilon: 0.084294\n",
      "Episode: 17, step: 100, total_reward: -1.000000, epsilon: 0.083451\n",
      "Episode: 18, step: 57, total_reward: -1.560000, epsilon: 0.082617\n",
      "Episode: 19, step: 100, total_reward: -1.000000, epsilon: 0.081791\n",
      "Episode: 20, step: 45, total_reward: -1.440000, epsilon: 0.080973\n",
      "Episode: 21, step: 100, total_reward: -1.000000, epsilon: 0.080163\n",
      "Episode: 22, step: 100, total_reward: -1.000000, epsilon: 0.079361\n",
      "Episode: 23, step: 14, total_reward: -1.130000, epsilon: 0.078568\n",
      "Episode: 24, step: 23, total_reward: -1.220000, epsilon: 0.077782\n",
      "Episode: 25, step: 100, total_reward: -1.000000, epsilon: 0.077004\n",
      "Episode: 26, step: 100, total_reward: -1.000000, epsilon: 0.076234\n",
      "Episode: 27, step: 100, total_reward: -1.000000, epsilon: 0.075472\n",
      "Episode: 28, step: 51, total_reward: -1.500000, epsilon: 0.074717\n",
      "Episode: 29, step: 18, total_reward: -1.170000, epsilon: 0.073970\n",
      "Episode: 30, step: 6, total_reward: 0.950000, epsilon: 0.073230\n",
      "[0, 4, 8, 9, 13, 14, 15]\n",
      "Episode: 31, step: 10, total_reward: -1.090000, epsilon: 0.072498\n",
      "Episode: 32, step: 3, total_reward: -1.020000, epsilon: 0.071773\n",
      "Episode: 33, step: 3, total_reward: -1.020000, epsilon: 0.071055\n",
      "Episode: 34, step: 3, total_reward: -1.020000, epsilon: 0.070345\n",
      "Episode: 35, step: 2, total_reward: -1.010000, epsilon: 0.069641\n",
      "Episode: 36, step: 4, total_reward: -1.030000, epsilon: 0.068945\n",
      "Episode: 37, step: 5, total_reward: -1.040000, epsilon: 0.068255\n",
      "Episode: 38, step: 10, total_reward: -1.090000, epsilon: 0.067573\n",
      "Episode: 39, step: 3, total_reward: -1.020000, epsilon: 0.066897\n",
      "Episode: 40, step: 3, total_reward: -1.020000, epsilon: 0.066228\n",
      "Episode: 41, step: 4, total_reward: -1.030000, epsilon: 0.065566\n",
      "Episode: 42, step: 100, total_reward: -1.000000, epsilon: 0.064910\n",
      "Episode: 43, step: 100, total_reward: -1.000000, epsilon: 0.064261\n",
      "Episode: 44, step: 100, total_reward: -1.000000, epsilon: 0.063619\n",
      "Episode: 45, step: 10, total_reward: -1.090000, epsilon: 0.062982\n",
      "Episode: 46, step: 2, total_reward: -1.010000, epsilon: 0.062353\n",
      "Episode: 47, step: 10, total_reward: -1.090000, epsilon: 0.061729\n",
      "Episode: 48, step: 46, total_reward: -1.450000, epsilon: 0.061112\n",
      "Episode: 49, step: 2, total_reward: -1.010000, epsilon: 0.060501\n",
      "Episode: 50, step: 4, total_reward: -1.030000, epsilon: 0.059896\n",
      "Episode: 51, step: 2, total_reward: -1.010000, epsilon: 0.059297\n",
      "Episode: 52, step: 3, total_reward: -1.020000, epsilon: 0.058704\n",
      "Episode: 53, step: 2, total_reward: -1.010000, epsilon: 0.058117\n",
      "Episode: 54, step: 92, total_reward: -1.910000, epsilon: 0.057535\n",
      "Episode: 55, step: 100, total_reward: -1.000000, epsilon: 0.056960\n",
      "Episode: 56, step: 100, total_reward: -1.000000, epsilon: 0.056391\n",
      "Episode: 57, step: 16, total_reward: -1.150000, epsilon: 0.055827\n",
      "Episode: 58, step: 18, total_reward: -1.170000, epsilon: 0.055268\n",
      "Episode: 59, step: 70, total_reward: -1.690000, epsilon: 0.054716\n",
      "Episode: 60, step: 8, total_reward: -1.070000, epsilon: 0.054169\n",
      "Episode: 61, step: 31, total_reward: -1.300000, epsilon: 0.053627\n",
      "Episode: 62, step: 15, total_reward: -1.140000, epsilon: 0.053091\n",
      "Episode: 63, step: 25, total_reward: -1.240000, epsilon: 0.052560\n",
      "Episode: 64, step: 32, total_reward: -1.310000, epsilon: 0.052034\n",
      "Episode: 65, step: 5, total_reward: -1.040000, epsilon: 0.051514\n",
      "Episode: 66, step: 100, total_reward: -1.000000, epsilon: 0.050999\n",
      "Episode: 67, step: 18, total_reward: -1.170000, epsilon: 0.050489\n",
      "Episode: 68, step: 14, total_reward: -1.130000, epsilon: 0.049984\n",
      "Episode: 69, step: 53, total_reward: -1.520000, epsilon: 0.049484\n",
      "Episode: 70, step: 34, total_reward: -1.330000, epsilon: 0.048989\n",
      "Episode: 71, step: 63, total_reward: -1.620000, epsilon: 0.048499\n",
      "Episode: 72, step: 31, total_reward: -1.300000, epsilon: 0.048014\n",
      "Episode: 73, step: 36, total_reward: -1.350000, epsilon: 0.047534\n",
      "Episode: 74, step: 100, total_reward: -1.000000, epsilon: 0.047059\n",
      "Episode: 75, step: 63, total_reward: -1.620000, epsilon: 0.046588\n",
      "Episode: 76, step: 100, total_reward: -1.000000, epsilon: 0.046122\n",
      "Episode: 77, step: 100, total_reward: -1.000000, epsilon: 0.045661\n",
      "Episode: 78, step: 100, total_reward: -1.000000, epsilon: 0.045204\n",
      "Episode: 79, step: 97, total_reward: -1.960000, epsilon: 0.044752\n",
      "Episode: 80, step: 100, total_reward: -1.000000, epsilon: 0.044305\n",
      "Episode: 81, step: 100, total_reward: -1.000000, epsilon: 0.043862\n",
      "Episode: 82, step: 10, total_reward: -1.090000, epsilon: 0.043423\n",
      "Episode: 83, step: 13, total_reward: -1.120000, epsilon: 0.042989\n",
      "Episode: 84, step: 100, total_reward: -1.000000, epsilon: 0.042559\n",
      "Episode: 85, step: 31, total_reward: -1.300000, epsilon: 0.042133\n",
      "Episode: 86, step: 100, total_reward: -1.000000, epsilon: 0.041712\n",
      "Episode: 87, step: 100, total_reward: -1.000000, epsilon: 0.041295\n",
      "Episode: 88, step: 100, total_reward: -1.000000, epsilon: 0.040882\n",
      "Episode: 89, step: 100, total_reward: -1.000000, epsilon: 0.040473\n",
      "Episode: 90, step: 100, total_reward: -1.000000, epsilon: 0.040068\n",
      "Episode: 91, step: 100, total_reward: -1.000000, epsilon: 0.039668\n",
      "Episode: 92, step: 100, total_reward: -1.000000, epsilon: 0.039271\n",
      "Episode: 93, step: 100, total_reward: -1.000000, epsilon: 0.038878\n",
      "Episode: 94, step: 100, total_reward: -1.000000, epsilon: 0.038490\n",
      "Episode: 95, step: 88, total_reward: -1.870000, epsilon: 0.038105\n",
      "Episode: 96, step: 100, total_reward: -1.000000, epsilon: 0.037724\n",
      "Episode: 97, step: 100, total_reward: -1.000000, epsilon: 0.037346\n",
      "Episode: 98, step: 100, total_reward: -1.000000, epsilon: 0.036973\n",
      "Episode: 99, step: 100, total_reward: -1.000000, epsilon: 0.036603\n",
      "min step:  6 min step episode 30\n",
      "shortest path:  [0, 4, 8, 9, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "max_episodes=100\n",
    "max_step=100\n",
    "sess.run(init)\n",
    "# initialize\n",
    "\n",
    "epsilon=epsilon0\n",
    "min_step=max_step\n",
    "min_step_episode=max_episodes\n",
    "min_step_path=[]\n",
    "for episode in range(max_episodes):\n",
    "    epsilon=epsilon*0.99\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state_list = [state]\n",
    "    win=0\n",
    "\n",
    "    for step in range(1,max_step+1):\n",
    "        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m[0][state]=1.0\n",
    "        Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "        if np.random.rand()>epsilon :\n",
    "            action=np.argmax(Q_pred[0])\n",
    "        else:\n",
    "            action=np.random.choice(env.action_space.n)\n",
    "        \n",
    "        state_next,reward,game_over,_ = env.step(action)\n",
    "        \n",
    "        if reward==0 : \n",
    "            reward=-0.01\n",
    "        if reward!=1.0 and game_over==1:\n",
    "            reward=-1.0\n",
    "            \n",
    "        state_m_next=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "        state_m_next[0][state_next]=1.0\n",
    "        Q_pred_next=sess.run(Y_pred, feed_dict={X:state_m_next}) \n",
    "\n",
    "        if np.random.rand()>epsilon :\n",
    "            action_next=np.argmax(Q_pred_next[0])\n",
    "        else:\n",
    "            action_next=np.random.choice(env.action_space.n)\n",
    "\n",
    "        Y_true=copy.deepcopy(Q_pred[0])\n",
    "\n",
    "        if reward==1.0 and game_over==1:\n",
    "            Y_true[action]= reward\n",
    "        else :\n",
    "            Y_true[action]=reward+gamma* (Q_pred_next[0][action_next])\n",
    "        Y_true=np.reshape(Y_true,[1,env.action_space.n])\n",
    "        for j in range(0,1):\n",
    "            sess.run([opt],feed_dict={X:state_m,Y:Y_true})\n",
    "            cost= sess.run([loss],feed_dict={X:state_m,Y:Y_true})\n",
    "#            print(cost)\n",
    "            \n",
    "        \n",
    "        \n",
    "#        print(state, Q_pred,Y_true,action)\n",
    "#        state_m=np.zeros([1,env.observation_space.n],dtype=np.float32)\n",
    "#        state_m[0][state]=1.0\n",
    "#        Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "#        cost= sess.run([loss],feed_dict={X:state_m,Y:Y_true})\n",
    "#        print(Q_pred,cost)\n",
    "        state=state_next\n",
    "\n",
    "        total_reward=total_reward+reward\n",
    "        state_list.append(state)\n",
    "        if game_over and reward==1:\n",
    "            win=1\n",
    "            if step< min_step:\n",
    "                min_step=step\n",
    "                min_step_path=state_list\n",
    "                min_step_episode=episode\n",
    "        if game_over==1:\n",
    "            break\n",
    "\n",
    "\n",
    "    line_out=\"Episode: %d, step: %d, total_reward: %f, epsilon: %f\" %(episode, step, total_reward,epsilon)\n",
    "    print(line_out)\n",
    "    if reward==1 :\n",
    "        print(state_list)\n",
    "print( \"min step: \", min_step, \"min step episode\", min_step_episode)\n",
    "print(\"shortest path: \", min_step_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[-0.10371144 -0.04718725 -0.39003986 -0.13314784]\n",
      " [-0.01295955 -0.31059718 -0.268078   -0.5292789 ]\n",
      " [ 0.00252811  0.02393141 -0.56823885 -0.14255147]\n",
      " [ 0.23731808 -0.11690845 -0.65142334 -0.2535934 ]\n",
      " [-0.1563792  -0.14623381 -0.6349176  -0.04615644]\n",
      " [ 0.09439113 -0.42676872 -0.15013653 -0.06474429]\n",
      " [ 0.01732587 -0.0743316  -1.1717143  -0.45566872]\n",
      " [-0.09400165 -0.1753064  -0.04028922 -0.19454819]\n",
      " [ 0.01642446 -0.17076515 -0.01183125 -0.01332718]\n",
      " [-0.04719368  0.03110484 -0.2719726  -0.17909639]\n",
      " [ 0.2591393  -0.24129577 -0.6661645  -0.51256675]\n",
      " [-0.05958344 -0.20918868 -0.2826138  -0.08176109]\n",
      " [-0.05512813 -0.24832086 -0.34122327 -0.23119436]\n",
      " [-0.2906648  -0.7003897  -0.23363088  0.37964177]\n",
      " [ 0.6378388  -0.6354533   0.47216693 -0.245102  ]\n",
      " [ 0.02261345  0.12780623 -0.12614343 -0.23584908]]\n"
     ]
    }
   ],
   "source": [
    "#state=np.arange(0,15)\n",
    "#sess.run(init)\n",
    "state_m=np.zeros([16,env.observation_space.n],dtype=np.float32)\n",
    "for i in range(0,15):\n",
    "    state_m[i][i]=1.0\n",
    "print(state_m)\n",
    "Q_pred=sess.run(Y_pred,feed_dict={X:state_m})\n",
    "print(Q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
